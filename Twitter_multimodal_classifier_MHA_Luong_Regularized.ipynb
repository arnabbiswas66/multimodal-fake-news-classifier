{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnabbiswas66/multimodal-fake-news-classifier/blob/main/Twitter_multimodal_classifier_MHA_Luong_Regularized.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AECm9YZxm322",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d23ff21c-7a48-456c-c55c-493a28094c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/6.0 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#!pip install -q -U \"tensorflow-text==2.11.*\"\n",
        "!pip install -q tensorflow_text\n",
        "#!pip install -q talos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q talos"
      ],
      "metadata": {
        "id": "hdp9PdFYyRyz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "683bebc1-4743-434f-cbee-60057423a15f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for chances (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S0_Dwt-ondpi"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.platform.tf_logging import warn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import re\n",
        "from os import listdir\n",
        "import shutil\n",
        "import glob\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import talos as ta\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x20211H_nnLR",
        "outputId": "f26436e3-da3e-4f79-83be-815f8fecb0e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BbHqdujoUn87"
      },
      "outputs": [],
      "source": [
        "BASE_SAVE_LOCATION = \"/content/drive/MyDrive/multimodal-news\"\n",
        "CHECKPOINT_FILEPATH = '/content/drive/MyDrive/multimodal-news/twitter-models/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKusTHhYWBlz"
      },
      "source": [
        "## Load the dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JftdaRkIWAAo",
        "outputId": "0752d8e4-30ef-4efa-d790-980e40b264c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14258, 3) (1923, 3)\n"
          ]
        }
      ],
      "source": [
        "#Load training df\n",
        "train_df_model = pd.read_pickle(BASE_SAVE_LOCATION+\"/twitter-train.pickle\")\n",
        "test_df_model = pd.read_pickle(BASE_SAVE_LOCATION+\"/twitter-test.pickle\")\n",
        "print(train_df_model.shape, test_df_model.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "g3iaPhB7gqdq"
      },
      "outputs": [],
      "source": [
        "# This is for Talos as it doesn't take TF DataSet as input\n",
        "X_train = train_df_model.copy()\n",
        "y_train = X_train.pop('label')\n",
        "\n",
        "X_test = test_df_model.copy()\n",
        "y_test = X_test.pop('label')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCuEsRz4CIdR"
      },
      "source": [
        "# Data input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sjc2FLKhBfbj"
      },
      "outputs": [],
      "source": [
        "# Define TF Hub paths to the BERT encoder and its preprocessor\n",
        "bert_model_path = (\n",
        "    \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1\"\n",
        ")\n",
        "bert_preprocess_path = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uZzOdgKYInBJ"
      },
      "outputs": [],
      "source": [
        "def make_bert_preprocessing_model(sentence_features, seq_length=128):\n",
        "    \"\"\"Returns Model mapping string features to BERT inputs.\n",
        "\n",
        "  Args:\n",
        "    sentence_features: A list with the names of string-valued features.\n",
        "    seq_length: An integer that defines the sequence length of BERT inputs.\n",
        "\n",
        "  Returns:\n",
        "    A Keras Model that can be called on a list or dict of string Tensors\n",
        "    (with the order or names, resp., given by sentence_features) and\n",
        "    returns a dict of tensors for input to BERT.\n",
        "  \"\"\"\n",
        "\n",
        "    input_segments = [\n",
        "        tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
        "        for ft in sentence_features\n",
        "    ]\n",
        "\n",
        "    # Tokenize the text to word pieces.\n",
        "    bert_preprocess = hub.load(bert_preprocess_path)\n",
        "    tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name=\"tokenizer\")\n",
        "    segments = [tokenizer(s) for s in input_segments]\n",
        "\n",
        "    # Optional: Trim segments in a smart way to fit seq_length.\n",
        "    # Simple cases (like this example) can skip this step and let\n",
        "    # the next step apply a default truncation to approximately equal lengths.\n",
        "    truncated_segments = segments\n",
        "\n",
        "    # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
        "    # are model-dependent, so this gets loaded from the SavedModel.\n",
        "    packer = hub.KerasLayer(\n",
        "        bert_preprocess.bert_pack_inputs,\n",
        "        arguments=dict(seq_length=seq_length),\n",
        "        name=\"packer\",\n",
        "    )\n",
        "    model_inputs = packer(truncated_segments)\n",
        "    return keras.Model(input_segments, model_inputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqW4X6onJQKQ",
        "outputId": "53d92722-7179-473e-afff-6f7edb3e3df2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_1 (InputLayer)         [(None,)]                 0         \n",
            "                                                                 \n",
            " tokenizer (KerasLayer)      (None, None, None)        0         \n",
            "                                                                 \n",
            " packer (KerasLayer)         {'input_word_ids': (None  0         \n",
            "                             , 128),                             \n",
            "                              'input_type_ids': (None            \n",
            "                             , 128),                             \n",
            "                              'input_mask': (None, 12            \n",
            "                             8)}                                 \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 0\n",
            "Trainable params: 0\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "bert_preprocess_model = make_bert_preprocessing_model([\"text_1\"])\n",
        "bert_preprocess_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCOmGQXtJn9a",
        "outputId": "0c1934de-8135-4b6f-bf56-fa4f039cdde2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1: #newyork #frankestorm #hurricane #nature #sky #amazing #usa #nyc http://t.co/rjceaPp1\n",
            "Text 2: #newyork #frankestorm #hurricane #nature #sky #amazing #usa #nyc http://t.co/rjceaPp1\n",
            "Keys           :  ['input_word_ids', 'input_type_ids', 'input_mask']\n",
            "Shape Word Ids :  (1, 128)\n",
            "Word Ids       :  tf.Tensor(\n",
            "[ 101 1001 2047 7677 8024 1001 3581 4355 2953 2213 1001 7064 1001 3267\n",
            " 1001 3712], shape=(16,), dtype=int32)\n",
            "Shape Mask     :  (1, 128)\n",
            "Input Mask     :  tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(16,), dtype=int32)\n",
            "Shape Type Ids :  (1, 128)\n",
            "Type Ids       :  tf.Tensor([0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(16,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "idx = np.random.choice(len(train_df_model))\n",
        "row = train_df_model.iloc[idx]\n",
        "sample_text_1, sample_text_2 = row[\"tweetText\"], row[\"tweetText\"]\n",
        "print(f\"Text 1: {sample_text_1}\")\n",
        "print(f\"Text 2: {sample_text_2}\")\n",
        "\n",
        "test_text = [np.array([sample_text_1])]\n",
        "text_preprocessed = bert_preprocess_model(test_text)\n",
        "\n",
        "print(\"Keys           : \", list(text_preprocessed.keys()))\n",
        "print(\"Shape Word Ids : \", text_preprocessed[\"input_word_ids\"].shape)\n",
        "print(\"Word Ids       : \", text_preprocessed[\"input_word_ids\"][0, :16])\n",
        "print(\"Shape Mask     : \", text_preprocessed[\"input_mask\"].shape)\n",
        "print(\"Input Mask     : \", text_preprocessed[\"input_mask\"][0, :16])\n",
        "print(\"Shape Type Ids : \", text_preprocessed[\"input_type_ids\"].shape)\n",
        "print(\"Type Ids       : \", text_preprocessed[\"input_type_ids\"][0, :16])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dhn-DPb7Lsb1"
      },
      "outputs": [],
      "source": [
        "def dataframe_to_dataset(dataframe):\n",
        "    columns = ['tweetText', 'image_1', 'label']\n",
        "    dataframe = dataframe[columns].copy()\n",
        "    labels = dataframe.pop(\"label\")\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO6IRl9rkfT4"
      },
      "source": [
        "## Preprocessing utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "slef7sY9ke1l"
      },
      "outputs": [],
      "source": [
        "resize = (224, 224)\n",
        "bert_input_features = [\"input_word_ids\", \"input_type_ids\", \"input_mask\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FrTCJPUzkP3Q"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path):\n",
        "  extension = tf.strings.split(image_path,'.')[-1]\n",
        "  image = tf.io.read_file(image_path)\n",
        "  if extension == b\"gif\":\n",
        "    image = tf.io.decode_image(image, 3, expand_animations=False)\n",
        "  elif extension == b\"png\":\n",
        "    image = tf.image.decode_png(image, 3)\n",
        "  else:\n",
        "    image = tf.image.decode_jpeg(image, 3)\n",
        "  image = tf.image.resize(image, resize)\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IvgkLXK5k5ZX"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text_1):\n",
        "  text_1 = tf.convert_to_tensor([text_1])\n",
        "  output = bert_preprocess_model([text_1])\n",
        "  output = {feature: tf.squeeze(output[feature]) for feature in bert_input_features}\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uXQHweTHpCQM"
      },
      "outputs": [],
      "source": [
        "def preprocess_text_and_image(sample):\n",
        "  image_1 = preprocess_image(sample[\"image_1\"])\n",
        "  text = preprocess_text(sample[\"tweetText\"])\n",
        "  return {\"image_1\": image_1, \"text\": text}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "C-0aLqtupoL5"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "auto = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DwL2IjrIp-Tn"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(dataframe, training=True):\n",
        "  ds = dataframe_to_dataset(dataframe)\n",
        "  if training:\n",
        "      ds = ds.shuffle(len(train_df_model))\n",
        "  ds = ds.map(lambda x, y: (preprocess_text_and_image(x), y)).cache()\n",
        "  ds = ds.batch(batch_size).prefetch(auto)\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5rSseL-fqSCE"
      },
      "outputs": [],
      "source": [
        "train_ds = prepare_dataset(train_df_model)\n",
        "test_ds = prepare_dataset(test_df_model, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUA1fjql-NCs"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsnHD-at_70f"
      },
      "source": [
        "### Projection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JUmnESU36Q-N"
      },
      "outputs": [],
      "source": [
        "def project_embeddings(\n",
        "    embeddings, num_projection_layers, projection_dims, dropout_rate\n",
        "):\n",
        "    projected_embeddings = keras.layers.Dense(units=projection_dims)(embeddings)\n",
        "    for _ in range(num_projection_layers):\n",
        "        x = tf.nn.gelu(projected_embeddings)\n",
        "        x = keras.layers.Dense(projection_dims)(x)\n",
        "        x = keras.layers.Dropout(dropout_rate)(x)\n",
        "        x = keras.layers.Add()([projected_embeddings, x])\n",
        "        projected_embeddings = keras.layers.LayerNormalization()(x)\n",
        "    return projected_embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEelNTYZAB0h"
      },
      "source": [
        "### Vision encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "y2ypqRWE_Oyq"
      },
      "outputs": [],
      "source": [
        "def create_vision_encoder(\n",
        "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
        "):\n",
        "    # Load the pre-trained ResNet50V2 model to be used as the base encoder.\n",
        "    resnet_v2 = keras.applications.EfficientNetV2B3(\n",
        "        include_top=False, weights=\"imagenet\", pooling=\"avg\"\n",
        "    )\n",
        "    # Set the trainability of the base encoder.\n",
        "    for layer in resnet_v2.layers:\n",
        "        layer.trainable = trainable\n",
        "\n",
        "    # Receive the images as inputs.\n",
        "    image_1 = keras.Input(shape=(224, 224, 3), name=\"image_1\")\n",
        "    \n",
        "    # Preprocess the input image.\n",
        "    preprocessed_1 = keras.applications.resnet_v2.preprocess_input(image_1)\n",
        "    \n",
        "    # Generate the embeddings for the images using the resnet_v2 model\n",
        "    # concatenate them.\n",
        "    embeddings = resnet_v2(preprocessed_1)\n",
        "    #embeddings = keras.layers.Concatenate()([embeddings_1, embeddings_2])\n",
        "\n",
        "    # Project the embeddings produced by the model.\n",
        "    outputs = project_embeddings(\n",
        "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
        "    )\n",
        "    # Create the vision encoder model.\n",
        "    return keras.Model([image_1], outputs, name=\"vision_encoder\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtChuDndAGLm"
      },
      "source": [
        "### Text Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "UoVTKoTt_1Pv"
      },
      "outputs": [],
      "source": [
        "def create_text_encoder(\n",
        "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
        "):\n",
        "    # Load the pre-trained BERT model to be used as the base encoder.\n",
        "    bert = hub.KerasLayer(bert_model_path, name=\"bert\",)\n",
        "    # Set the trainability of the base encoder.\n",
        "    bert.trainable = trainable\n",
        "\n",
        "    # Receive the text as inputs.\n",
        "    bert_input_features = [\"input_type_ids\", \"input_mask\", \"input_word_ids\"]\n",
        "    inputs = {\n",
        "        feature: keras.Input(shape=(128,), dtype=tf.int32, name=feature)\n",
        "        for feature in bert_input_features\n",
        "    }\n",
        "\n",
        "    # Generate embeddings for the preprocessed text using the BERT model.\n",
        "    embeddings = bert(inputs)[\"pooled_output\"]\n",
        "\n",
        "    # Project the embeddings produced by the model.\n",
        "    outputs = project_embeddings(\n",
        "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
        "    )\n",
        "    # Create the text encoder model.\n",
        "    return keras.Model(inputs, outputs, name=\"text_encoder\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF0jLUzqjwFv"
      },
      "source": [
        "### Multi Head Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "UX52i1Pj4piV"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = keras.layers.Dropout(rate)\n",
        "        self.dropout2 = keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, y, training):\n",
        "        attn_output = self.att(x, y)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugj-e4g8AWOp"
      },
      "source": [
        "## MultiModal model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "1RjaNwDnANoc"
      },
      "outputs": [],
      "source": [
        "def create_multimodal_model(\n",
        "    num_projection_layers=0,\n",
        "    projection_dims=224,\n",
        "    dropout_rate=0.1,\n",
        "    vision_trainable=False,\n",
        "    text_trainable=False,\n",
        "    attention=False\n",
        "):\n",
        "    # Receive the images as inputs.\n",
        "    image_1 = keras.Input(shape=(224, 224, 3), name=\"image_1\")\n",
        "    \n",
        "    # Receive the text as inputs.\n",
        "    bert_input_features = [\"input_type_ids\", \"input_mask\", \"input_word_ids\"]\n",
        "    text_inputs = {\n",
        "        feature: keras.Input(shape=(128,), dtype=tf.int32, name=feature)\n",
        "        for feature in bert_input_features\n",
        "    }\n",
        "\n",
        "    # Create the encoders.\n",
        "    vision_encoder = create_vision_encoder(\n",
        "        num_projection_layers, projection_dims, dropout_rate, vision_trainable\n",
        "    )\n",
        "    text_encoder = create_text_encoder(\n",
        "        num_projection_layers, projection_dims, dropout_rate, text_trainable\n",
        "    )\n",
        "\n",
        "    # Fetch the embedding projections.\n",
        "    vision_projections = vision_encoder([image_1])\n",
        "    vision_projections = keras.layers.Dropout(dropout_rate)(vision_projections)\n",
        "    text_projections = text_encoder(text_inputs)\n",
        "    text_projections = keras.layers.Dropout(dropout_rate)(text_projections)\n",
        "    \n",
        "    # Cross-attention.\n",
        "    if attention:\n",
        "      transformer_block = TransformerBlock(projection_dims, 4, projection_dims)\n",
        "      x = transformer_block(tf.expand_dims(vision_projections, -1), tf.expand_dims(text_projections, -1))\n",
        "      x = tf.keras.layers.Flatten()(x)\n",
        "      x = project_embeddings(\n",
        "        x, 1, projection_dims, dropout_rate) \n",
        "      query_value_attention_seq = keras.layers.Attention(use_scale=True, dropout=0.2)(\n",
        "            [vision_projections, text_projections]\n",
        "        )\n",
        "      \n",
        "    # Concatenate the projections and pass through the classification layer.\n",
        "    concatenated = keras.layers.Concatenate()([vision_projections, text_projections])\n",
        "    if attention:\n",
        "        concatenated = keras.layers.Concatenate()([concatenated, x, query_value_attention_seq])\n",
        "        #x = tf.keras.layers.Flatten(x)\n",
        "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(concatenated)\n",
        "    return keras.Model([image_1, text_inputs], outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "HYAqM03bYqpj"
      },
      "outputs": [],
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics= [\n",
        "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      tf.keras.metrics.Precision(name='precision'),\n",
        "      tf.keras.metrics.Recall(name='recall')\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzh_oeuuZVrA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvh6vbUTXRRs"
      },
      "source": [
        "## Final Multimodal model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMyfwuhtXQRJ"
      },
      "outputs": [],
      "source": [
        "def multimodal_model(X_train, y_train, X_test, y_test, params):\n",
        "  train_ds = prepare_dataset(train_df_model)\n",
        "  test_ds = prepare_dataset(test_df_model, False)\n",
        "  model = create_multimodal_model(params['num_projection_layers'],\n",
        "    params['projection_dims'],\n",
        "    params['dropout_rate'],\n",
        "    params['vision_trainable'],\n",
        "    params['text_trainable'],\n",
        "    params['attention'])\n",
        "  model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(params['lr']), loss=loss, metrics=metrics\n",
        "  )\n",
        "  history = model.fit(\n",
        "      train_ds, validation_data=test_ds, \n",
        "      epochs=params['epochs'], batch_size=params['batch_size'])\n",
        "  return history, model\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hLDJku6bY9O"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'num_projection_layers' : [0],\n",
        "    'projection_dims' : [128, 224],\n",
        "    'dropout_rate' : [0.1, 0.2],\n",
        "    'vision_trainable' : [False],\n",
        "    'text_trainable' : [False],\n",
        "    'attention' : [True],\n",
        "    'lr' : [0.001, 0.0005],\n",
        "    'epochs' : [10],\n",
        "    'batch_size' : [32, 64]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhrpIETIevhH",
        "outputId": "79224f02-f43c-4d89-c097-fc20bc27ea89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/16 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_projection_layers': 0, 'projection_dims': 128, 'dropout_rate': 0.1, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.001, 'epochs': 10, 'batch_size': 32}\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b3_notop.h5\n",
            "52606240/52606240 [==============================] - 3s 0us/step\n",
            "Epoch 1/10\n",
            "446/446 [==============================] - 383s 763ms/step - loss: 0.5123 - accuracy: 0.7680 - precision: 0.7019 - recall: 0.5660 - val_loss: 0.4329 - val_accuracy: 0.8294 - val_precision: 0.8347 - val_recall: 0.9080\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 52s 117ms/step - loss: 0.3573 - accuracy: 0.8514 - precision: 0.8119 - recall: 0.7392 - val_loss: 0.7081 - val_accuracy: 0.5642 - val_precision: 0.9165 - val_recall: 0.3364\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 53s 118ms/step - loss: 0.3124 - accuracy: 0.8681 - precision: 0.8357 - recall: 0.7674 - val_loss: 0.5549 - val_accuracy: 0.7639 - val_precision: 0.9414 - val_recall: 0.6653\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 53s 120ms/step - loss: 0.2864 - accuracy: 0.8832 - precision: 0.8549 - recall: 0.7953 - val_loss: 0.6544 - val_accuracy: 0.6401 - val_precision: 0.9357 - val_recall: 0.4582\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 54s 120ms/step - loss: 0.2749 - accuracy: 0.8887 - precision: 0.8605 - recall: 0.8073 - val_loss: 0.4763 - val_accuracy: 0.8534 - val_precision: 0.9263 - val_recall: 0.8326\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2628 - accuracy: 0.8930 - precision: 0.8671 - recall: 0.8139 - val_loss: 0.6412 - val_accuracy: 0.7051 - val_precision: 0.9444 - val_recall: 0.5634\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2441 - accuracy: 0.8986 - precision: 0.8715 - recall: 0.8271 - val_loss: 0.6816 - val_accuracy: 0.6563 - val_precision: 0.9446 - val_recall: 0.4805\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 54s 120ms/step - loss: 0.2384 - accuracy: 0.9050 - precision: 0.8812 - recall: 0.8365 - val_loss: 0.4509 - val_accuracy: 0.8710 - val_precision: 0.9562 - val_recall: 0.8326\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2383 - accuracy: 0.9023 - precision: 0.8781 - recall: 0.8314 - val_loss: 0.5822 - val_accuracy: 0.7852 - val_precision: 0.9563 - val_recall: 0.6893\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2298 - accuracy: 0.9069 - precision: 0.8854 - recall: 0.8379 - val_loss: 0.5708 - val_accuracy: 0.7956 - val_precision: 0.9583 - val_recall: 0.7051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▋         | 1/16 [15:08<3:47:14, 908.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_projection_layers': 0, 'projection_dims': 128, 'dropout_rate': 0.1, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.001, 'epochs': 10, 'batch_size': 64}\n",
            "Epoch 1/10\n",
            "446/446 [==============================] - 105s 200ms/step - loss: 0.4749 - accuracy: 0.7901 - precision: 0.7823 - recall: 0.6157 - val_loss: 0.5123 - val_accuracy: 0.8248 - val_precision: 0.9207 - val_recall: 0.7887\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 53s 120ms/step - loss: 0.3414 - accuracy: 0.8578 - precision: 0.8203 - recall: 0.7511 - val_loss: 0.5712 - val_accuracy: 0.7239 - val_precision: 0.9322 - val_recall: 0.6040\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 54s 120ms/step - loss: 0.2996 - accuracy: 0.8792 - precision: 0.8494 - recall: 0.7886 - val_loss: 0.5979 - val_accuracy: 0.7077 - val_precision: 0.9436 - val_recall: 0.5684\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2750 - accuracy: 0.8870 - precision: 0.8565 - recall: 0.8067 - val_loss: 0.4712 - val_accuracy: 0.8440 - val_precision: 0.9512 - val_recall: 0.7920\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2723 - accuracy: 0.8886 - precision: 0.8601 - recall: 0.8073 - val_loss: 0.7867 - val_accuracy: 0.5959 - val_precision: 0.9300 - val_recall: 0.3853\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2556 - accuracy: 0.8973 - precision: 0.8702 - recall: 0.8245 - val_loss: 0.8608 - val_accuracy: 0.5689 - val_precision: 0.9315 - val_recall: 0.3380\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2385 - accuracy: 0.9055 - precision: 0.8835 - recall: 0.8353 - val_loss: 0.5521 - val_accuracy: 0.8144 - val_precision: 0.9550 - val_recall: 0.7390\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2353 - accuracy: 0.9066 - precision: 0.8840 - recall: 0.8387 - val_loss: 0.7180 - val_accuracy: 0.6615 - val_precision: 0.9455 - val_recall: 0.4888\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2306 - accuracy: 0.9087 - precision: 0.8900 - recall: 0.8381 - val_loss: 0.4863 - val_accuracy: 0.8388 - val_precision: 0.9526 - val_recall: 0.7821\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2244 - accuracy: 0.9101 - precision: 0.8887 - recall: 0.8444 - val_loss: 0.4893 - val_accuracy: 0.8378 - val_precision: 0.9562 - val_recall: 0.7771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▎        | 2/16 [25:08<2:49:36, 726.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_projection_layers': 0, 'projection_dims': 128, 'dropout_rate': 0.1, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.0005, 'epochs': 10, 'batch_size': 32}\n",
            "Epoch 1/10\n",
            "446/446 [==============================] - 106s 201ms/step - loss: 0.5123 - accuracy: 0.7665 - precision: 0.7533 - recall: 0.5679 - val_loss: 0.4781 - val_accuracy: 0.7447 - val_precision: 0.7146 - val_recall: 0.9876\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 53s 120ms/step - loss: 0.3733 - accuracy: 0.8419 - precision: 0.8039 - recall: 0.7148 - val_loss: 0.4305 - val_accuracy: 0.7988 - val_precision: 0.7812 - val_recall: 0.9437\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.3277 - accuracy: 0.8633 - precision: 0.8282 - recall: 0.7604 - val_loss: 0.4304 - val_accuracy: 0.8445 - val_precision: 0.8498 - val_recall: 0.9138\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.3020 - accuracy: 0.8761 - precision: 0.8454 - recall: 0.7829 - val_loss: 0.4065 - val_accuracy: 0.8565 - val_precision: 0.8745 - val_recall: 0.9006\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2892 - accuracy: 0.8785 - precision: 0.8477 - recall: 0.7886 - val_loss: 0.4338 - val_accuracy: 0.8658 - val_precision: 0.9025 - val_recall: 0.8815\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 54s 120ms/step - loss: 0.2733 - accuracy: 0.8894 - precision: 0.8625 - recall: 0.8071 - val_loss: 0.3833 - val_accuracy: 0.8851 - val_precision: 0.9228 - val_recall: 0.8915\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 54s 120ms/step - loss: 0.2591 - accuracy: 0.8944 - precision: 0.8697 - recall: 0.8151 - val_loss: 0.4233 - val_accuracy: 0.8762 - val_precision: 0.9457 - val_recall: 0.8517\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2498 - accuracy: 0.8998 - precision: 0.8768 - recall: 0.8245 - val_loss: 0.3608 - val_accuracy: 0.8851 - val_precision: 0.9280 - val_recall: 0.8857\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 53s 120ms/step - loss: 0.2363 - accuracy: 0.9067 - precision: 0.8877 - recall: 0.8345 - val_loss: 0.3664 - val_accuracy: 0.8679 - val_precision: 0.8928 - val_recall: 0.8973\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 53s 120ms/step - loss: 0.2369 - accuracy: 0.9076 - precision: 0.8868 - recall: 0.8385 - val_loss: 0.3599 - val_accuracy: 0.8653 - val_precision: 0.8854 - val_recall: 0.9022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▉        | 3/16 [35:36<2:27:40, 681.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_projection_layers': 0, 'projection_dims': 128, 'dropout_rate': 0.1, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.0005, 'epochs': 10, 'batch_size': 64}\n",
            "Epoch 1/10\n",
            "446/446 [==============================] - 106s 202ms/step - loss: 0.5207 - accuracy: 0.7637 - precision: 0.7369 - recall: 0.5825 - val_loss: 0.5045 - val_accuracy: 0.8180 - val_precision: 0.9506 - val_recall: 0.7490\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 53s 120ms/step - loss: 0.3725 - accuracy: 0.8435 - precision: 0.8095 - recall: 0.7129 - val_loss: 0.7011 - val_accuracy: 0.4867 - val_precision: 0.8767 - val_recall: 0.2121\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.3323 - accuracy: 0.8611 - precision: 0.8259 - recall: 0.7554 - val_loss: 0.6024 - val_accuracy: 0.6828 - val_precision: 0.9332 - val_recall: 0.5327\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.3042 - accuracy: 0.8735 - precision: 0.8421 - recall: 0.7784 - val_loss: 0.6808 - val_accuracy: 0.6115 - val_precision: 0.9291 - val_recall: 0.4126\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 54s 120ms/step - loss: 0.2842 - accuracy: 0.8829 - precision: 0.8536 - recall: 0.7963 - val_loss: 0.4626 - val_accuracy: 0.8742 - val_precision: 0.9423 - val_recall: 0.8517\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2652 - accuracy: 0.8947 - precision: 0.8726 - recall: 0.8126 - val_loss: 0.5552 - val_accuracy: 0.7863 - val_precision: 0.9533 - val_recall: 0.6935\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2577 - accuracy: 0.8968 - precision: 0.8720 - recall: 0.8206 - val_loss: 0.4472 - val_accuracy: 0.8716 - val_precision: 0.9436 - val_recall: 0.8459\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2495 - accuracy: 0.8991 - precision: 0.8741 - recall: 0.8255 - val_loss: 0.5308 - val_accuracy: 0.8274 - val_precision: 0.9572 - val_recall: 0.7589\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2393 - accuracy: 0.9029 - precision: 0.8780 - recall: 0.8336 - val_loss: 0.4714 - val_accuracy: 0.8653 - val_precision: 0.9566 - val_recall: 0.8227\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2386 - accuracy: 0.9024 - precision: 0.8781 - recall: 0.8316 - val_loss: 0.3997 - val_accuracy: 0.8768 - val_precision: 0.9338 - val_recall: 0.8650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 4/16 [46:04<2:12:09, 660.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_projection_layers': 0, 'projection_dims': 128, 'dropout_rate': 0.2, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.001, 'epochs': 10, 'batch_size': 32}\n",
            "Epoch 1/10\n",
            "446/446 [==============================] - 107s 203ms/step - loss: 0.4982 - accuracy: 0.7837 - precision: 0.7533 - recall: 0.6355 - val_loss: 0.5641 - val_accuracy: 0.7634 - val_precision: 0.9372 - val_recall: 0.6678\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.3534 - accuracy: 0.8506 - precision: 0.8109 - recall: 0.7378 - val_loss: 0.5330 - val_accuracy: 0.7925 - val_precision: 0.9307 - val_recall: 0.7233\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.3085 - accuracy: 0.8724 - precision: 0.8391 - recall: 0.7782 - val_loss: 0.7931 - val_accuracy: 0.4805 - val_precision: 0.8881 - val_recall: 0.1972\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2937 - accuracy: 0.8770 - precision: 0.8435 - recall: 0.7888 - val_loss: 0.5022 - val_accuracy: 0.8430 - val_precision: 0.9511 - val_recall: 0.7904\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2819 - accuracy: 0.8861 - precision: 0.8569 - recall: 0.8031 - val_loss: 0.5713 - val_accuracy: 0.7161 - val_precision: 0.9436 - val_recall: 0.5824\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2636 - accuracy: 0.8926 - precision: 0.8688 - recall: 0.8102 - val_loss: 0.5278 - val_accuracy: 0.8144 - val_precision: 0.9541 - val_recall: 0.7399\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2522 - accuracy: 0.8968 - precision: 0.8711 - recall: 0.8214 - val_loss: 0.6802 - val_accuracy: 0.6448 - val_precision: 0.9411 - val_recall: 0.4631\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2429 - accuracy: 0.9029 - precision: 0.8818 - recall: 0.8287 - val_loss: 0.5251 - val_accuracy: 0.8320 - val_precision: 0.9595 - val_recall: 0.7647\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2334 - accuracy: 0.9072 - precision: 0.8872 - recall: 0.8367 - val_loss: 0.5924 - val_accuracy: 0.7540 - val_precision: 0.9531 - val_recall: 0.6396\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2388 - accuracy: 0.9034 - precision: 0.8764 - recall: 0.8371 - val_loss: 0.4820 - val_accuracy: 0.8502 - val_precision: 0.9590 - val_recall: 0.7954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███▏      | 5/16 [56:34<1:59:06, 649.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_projection_layers': 0, 'projection_dims': 128, 'dropout_rate': 0.2, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.001, 'epochs': 10, 'batch_size': 64}\n",
            "Epoch 1/10\n",
            "446/446 [==============================] - 108s 205ms/step - loss: 0.4855 - accuracy: 0.7841 - precision: 0.7619 - recall: 0.6230 - val_loss: 0.7174 - val_accuracy: 0.4815 - val_precision: 0.8777 - val_recall: 0.2022\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 53s 120ms/step - loss: 0.3522 - accuracy: 0.8506 - precision: 0.8087 - recall: 0.7411 - val_loss: 0.5419 - val_accuracy: 0.7832 - val_precision: 0.9399 - val_recall: 0.6993\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.3105 - accuracy: 0.8740 - precision: 0.8402 - recall: 0.7825 - val_loss: 0.6314 - val_accuracy: 0.6599 - val_precision: 0.9396 - val_recall: 0.4896\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2902 - accuracy: 0.8796 - precision: 0.8467 - recall: 0.7939 - val_loss: 0.5272 - val_accuracy: 0.8196 - val_precision: 0.9604 - val_recall: 0.7432\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 54s 122ms/step - loss: 0.2807 - accuracy: 0.8843 - precision: 0.8538 - recall: 0.8010 - val_loss: 0.5028 - val_accuracy: 0.8388 - val_precision: 0.9535 - val_recall: 0.7813\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 54s 122ms/step - loss: 0.2611 - accuracy: 0.8949 - precision: 0.8678 - recall: 0.8194 - val_loss: 0.4603 - val_accuracy: 0.8648 - val_precision: 0.9531 - val_recall: 0.8252\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 54s 122ms/step - loss: 0.2572 - accuracy: 0.8937 - precision: 0.8691 - recall: 0.8135 - val_loss: 0.4104 - val_accuracy: 0.8788 - val_precision: 0.9501 - val_recall: 0.8517\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 54s 122ms/step - loss: 0.2439 - accuracy: 0.9019 - precision: 0.8785 - recall: 0.8298 - val_loss: 0.6779 - val_accuracy: 0.6609 - val_precision: 0.9498 - val_recall: 0.4855\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 54s 122ms/step - loss: 0.2424 - accuracy: 0.9013 - precision: 0.8750 - recall: 0.8320 - val_loss: 0.5013 - val_accuracy: 0.8450 - val_precision: 0.9605 - val_recall: 0.7854\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 54s 122ms/step - loss: 0.2312 - accuracy: 0.9073 - precision: 0.8840 - recall: 0.8408 - val_loss: 0.4066 - val_accuracy: 0.8669 - val_precision: 0.9391 - val_recall: 0.8426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 6/16 [1:07:07<1:47:19, 643.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_projection_layers': 0, 'projection_dims': 128, 'dropout_rate': 0.2, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.0005, 'epochs': 10, 'batch_size': 32}\n",
            "Epoch 1/10\n",
            "446/446 [==============================] - 107s 204ms/step - loss: 0.5021 - accuracy: 0.7674 - precision: 0.7512 - recall: 0.5746 - val_loss: 0.5294 - val_accuracy: 0.8196 - val_precision: 0.9526 - val_recall: 0.7498\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 53s 119ms/step - loss: 0.3658 - accuracy: 0.8468 - precision: 0.8144 - recall: 0.7182 - val_loss: 0.6870 - val_accuracy: 0.5751 - val_precision: 0.9167 - val_recall: 0.3554\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.3340 - accuracy: 0.8585 - precision: 0.8221 - recall: 0.7511 - val_loss: 0.5951 - val_accuracy: 0.6698 - val_precision: 0.9320 - val_recall: 0.5112\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.3087 - accuracy: 0.8701 - precision: 0.8371 - recall: 0.7729 - val_loss: 0.6169 - val_accuracy: 0.6589 - val_precision: 0.9325 - val_recall: 0.4921\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2882 - accuracy: 0.8821 - precision: 0.8527 - recall: 0.7945 - val_loss: 0.6229 - val_accuracy: 0.6646 - val_precision: 0.9336 - val_recall: 0.5012\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 54s 120ms/step - loss: 0.2723 - accuracy: 0.8874 - precision: 0.8590 - recall: 0.8049 - val_loss: 0.6289 - val_accuracy: 0.6630 - val_precision: 0.9374 - val_recall: 0.4963\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2619 - accuracy: 0.8901 - precision: 0.8592 - recall: 0.8139 - val_loss: 0.4141 - val_accuracy: 0.8742 - val_precision: 0.9251 - val_recall: 0.8699\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2515 - accuracy: 0.9008 - precision: 0.8802 - recall: 0.8239 - val_loss: 0.4810 - val_accuracy: 0.8450 - val_precision: 0.9559 - val_recall: 0.7896\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2441 - accuracy: 0.9022 - precision: 0.8777 - recall: 0.8314 - val_loss: 0.4782 - val_accuracy: 0.8430 - val_precision: 0.9557 - val_recall: 0.7862\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2358 - accuracy: 0.9050 - precision: 0.8796 - recall: 0.8387 - val_loss: 0.4004 - val_accuracy: 0.8799 - val_precision: 0.9444 - val_recall: 0.8592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 7/16 [1:17:09<1:34:31, 630.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_projection_layers': 0, 'projection_dims': 128, 'dropout_rate': 0.2, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.0005, 'epochs': 10, 'batch_size': 64}\n",
            "Epoch 1/10\n",
            "446/446 [==============================] - 111s 215ms/step - loss: 0.5366 - accuracy: 0.7535 - precision: 0.7246 - recall: 0.5605 - val_loss: 0.4396 - val_accuracy: 0.8326 - val_precision: 0.9403 - val_recall: 0.7829\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 54s 120ms/step - loss: 0.3821 - accuracy: 0.8332 - precision: 0.7943 - recall: 0.6952 - val_loss: 0.4335 - val_accuracy: 0.8315 - val_precision: 0.8409 - val_recall: 0.9022\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.3442 - accuracy: 0.8565 - precision: 0.8190 - recall: 0.7482 - val_loss: 0.4249 - val_accuracy: 0.8310 - val_precision: 0.8382 - val_recall: 0.9056\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 55s 122ms/step - loss: 0.3145 - accuracy: 0.8664 - precision: 0.8295 - recall: 0.7698 - val_loss: 0.4240 - val_accuracy: 0.8809 - val_precision: 0.9187 - val_recall: 0.8890\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 55s 123ms/step - loss: 0.2992 - accuracy: 0.8760 - precision: 0.8466 - recall: 0.7810 - val_loss: 0.4123 - val_accuracy: 0.8814 - val_precision: 0.9216 - val_recall: 0.8865\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 55s 122ms/step - loss: 0.2757 - accuracy: 0.8875 - precision: 0.8611 - recall: 0.8024 - val_loss: 0.4159 - val_accuracy: 0.8528 - val_precision: 0.8696 - val_recall: 0.9006\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 55s 122ms/step - loss: 0.2651 - accuracy: 0.8904 - precision: 0.8614 - recall: 0.8120 - val_loss: 0.3939 - val_accuracy: 0.8606 - val_precision: 0.8808 - val_recall: 0.8998\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - ETA: 0s - loss: 0.2589 - accuracy: 0.8928 - precision: 0.8662 - recall: 0.8143"
          ]
        }
      ],
      "source": [
        "h = ta.Scan(x = X_train, y= y_train, params = params, model = multimodal_model, x_val = X_test, y_val = y_test, experiment_name = 'multi1', save_weights=False, print_params=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'num_projection_layers' : [1],\n",
        "    'projection_dims' : [128, 224],\n",
        "    'dropout_rate' : [0.1, 0.2],\n",
        "    'vision_trainable' : [False],\n",
        "    'text_trainable' : [False],\n",
        "    'attention' : [True],\n",
        "    'lr' : [0.001, 0.0005],\n",
        "    'epochs' : [10],\n",
        "    'batch_size' : [32]\n",
        "}"
      ],
      "metadata": {
        "id": "wC_IOmzcm2BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = ta.Scan(x = X_train, y= y_train, params = params, model = multimodal_model, x_val = X_test, y_val = y_test, experiment_name = 'multi1', save_weights=False, print_params=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNyODIYMm-mo",
        "outputId": "495afa3f-c151-4574-b752-01a4e2e04726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/8 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_projection_layers': 1, 'projection_dims': 128, 'dropout_rate': 0.1, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.001, 'epochs': 10, 'batch_size': 32}\n",
            "Epoch 1/10\n",
            "446/446 [==============================] - 120s 208ms/step - loss: 0.4790 - accuracy: 0.7883 - precision: 0.7164 - recall: 0.6365 - val_loss: 0.4602 - val_accuracy: 0.8346 - val_precision: 0.9397 - val_recall: 0.7871\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 57s 127ms/step - loss: 0.3078 - accuracy: 0.8746 - precision: 0.8324 - recall: 0.7957 - val_loss: 0.5246 - val_accuracy: 0.7400 - val_precision: 0.9164 - val_recall: 0.6446\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 57s 129ms/step - loss: 0.2658 - accuracy: 0.8926 - precision: 0.8580 - recall: 0.8241 - val_loss: 0.4445 - val_accuracy: 0.8383 - val_precision: 0.8972 - val_recall: 0.8384\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 58s 130ms/step - loss: 0.2417 - accuracy: 0.9045 - precision: 0.8772 - recall: 0.8400 - val_loss: 0.4927 - val_accuracy: 0.8076 - val_precision: 0.8893 - val_recall: 0.7920\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 58s 130ms/step - loss: 0.2144 - accuracy: 0.9150 - precision: 0.8903 - recall: 0.8587 - val_loss: 0.8454 - val_accuracy: 0.5377 - val_precision: 0.8750 - val_recall: 0.3074\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 58s 130ms/step - loss: 0.1980 - accuracy: 0.9228 - precision: 0.9030 - recall: 0.8689 - val_loss: 0.5192 - val_accuracy: 0.7592 - val_precision: 0.7735 - val_recall: 0.8716\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 58s 130ms/step - loss: 0.1970 - accuracy: 0.9226 - precision: 0.9039 - recall: 0.8671 - val_loss: 0.6551 - val_accuracy: 0.6745 - val_precision: 0.8797 - val_recall: 0.5576\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 58s 130ms/step - loss: 0.1761 - accuracy: 0.9318 - precision: 0.9153 - recall: 0.8834 - val_loss: 1.0252 - val_accuracy: 0.4899 - val_precision: 0.7989 - val_recall: 0.2502\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 58s 130ms/step - loss: 0.1642 - accuracy: 0.9374 - precision: 0.9248 - recall: 0.8903 - val_loss: 1.5750 - val_accuracy: 0.4685 - val_precision: 0.7577 - val_recall: 0.2254\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 58s 131ms/step - loss: 0.1586 - accuracy: 0.9393 - precision: 0.9258 - recall: 0.8954 - val_loss: 2.5752 - val_accuracy: 0.4394 - val_precision: 0.7295 - val_recall: 0.1698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▎        | 1/8 [10:52<1:16:05, 652.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_projection_layers': 1, 'projection_dims': 128, 'dropout_rate': 0.1, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.0005, 'epochs': 10, 'batch_size': 32}\n",
            "Epoch 1/10\n",
            "446/446 [==============================] - 107s 200ms/step - loss: 0.4708 - accuracy: 0.7456 - precision: 0.7220 - recall: 0.5311 - val_loss: 0.5192 - val_accuracy: 0.8242 - val_precision: 0.9550 - val_recall: 0.7556\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 58s 129ms/step - loss: 0.3308 - accuracy: 0.8614 - precision: 0.8151 - recall: 0.7723 - val_loss: 0.5360 - val_accuracy: 0.8232 - val_precision: 0.9549 - val_recall: 0.7539\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 58s 130ms/step - loss: 0.2863 - accuracy: 0.8812 - precision: 0.8415 - recall: 0.8065 - val_loss: 0.7739 - val_accuracy: 0.5299 - val_precision: 0.8895 - val_recall: 0.2867\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 58s 131ms/step - loss: 0.2506 - accuracy: 0.9015 - precision: 0.8731 - recall: 0.8349 - val_loss: 0.8268 - val_accuracy: 0.5757 - val_precision: 0.9186 - val_recall: 0.3554\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 58s 131ms/step - loss: 0.2449 - accuracy: 0.9030 - precision: 0.8750 - recall: 0.8377 - val_loss: 0.9972 - val_accuracy: 0.4457 - val_precision: 0.8373 - val_recall: 0.1450\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 58s 130ms/step - loss: 0.2265 - accuracy: 0.9113 - precision: 0.8848 - recall: 0.8532 - val_loss: 1.2990 - val_accuracy: 0.4155 - val_precision: 0.7943 - val_recall: 0.0928\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 58s 129ms/step - loss: 0.2144 - accuracy: 0.9172 - precision: 0.8945 - recall: 0.8610 - val_loss: 1.0238 - val_accuracy: 0.4561 - val_precision: 0.8286 - val_recall: 0.1682\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 58s 130ms/step - loss: 0.2020 - accuracy: 0.9189 - precision: 0.8970 - recall: 0.8632 - val_loss: 1.2393 - val_accuracy: 0.4373 - val_precision: 0.7990 - val_recall: 0.1384\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 58s 129ms/step - loss: 0.2004 - accuracy: 0.9186 - precision: 0.8966 - recall: 0.8628 - val_loss: 0.5422 - val_accuracy: 0.7769 - val_precision: 0.8220 - val_recall: 0.8227\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 58s 130ms/step - loss: 0.1821 - accuracy: 0.9280 - precision: 0.9100 - recall: 0.8777 - val_loss: 1.7901 - val_accuracy: 0.4124 - val_precision: 0.7484 - val_recall: 0.0961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 2/8 [22:19<1:07:15, 672.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_projection_layers': 1, 'projection_dims': 128, 'dropout_rate': 0.2, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.001, 'epochs': 10, 'batch_size': 32}\n",
            "Epoch 1/10\n",
            "125/446 [=======>......................] - ETA: 58s - loss: 0.6397 - accuracy: 0.5994 - precision: 0.5882 - recall: 0.2947"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best model seems to be with num_projection_layers': 0, 'projection_dims': 128, 'dropout_rate': 0.1, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.0005, 'epochs': 10, 'batch_size': 32"
      ],
      "metadata": {
        "id": "009j96udy_dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = '/content/drive/MyDrive/multimodal-news/twitter-models/model-comb3-weights.{val_accuracy:.4f}-{val_loss:.2f}.hdf5'\n",
        "save_model1 = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)"
      ],
      "metadata": {
        "id": "332pTb6ianDA"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multimodal_model = create_multimodal_model(attention=True, num_projection_layers=0, projection_dims=128, dropout_rate=0.2)\n",
        "multimodal_model.summary()"
      ],
      "metadata": {
        "id": "DG9HS8aNAuGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "151eb777-a56c-4f9c-c116-4b2de965945d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " image_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " input_mask (InputLayer)        [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " input_type_ids (InputLayer)    [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " input_word_ids (InputLayer)    [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " vision_encoder (Functional)    (None, 128)          13127358    ['image_1[0][0]']                \n",
            "                                                                                                  \n",
            " text_encoder (Functional)      (None, 128)          9623937     ['input_mask[0][0]',             \n",
            "                                                                  'input_type_ids[0][0]',         \n",
            "                                                                  'input_word_ids[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 128)          0           ['vision_encoder[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 128)          0           ['text_encoder[0][0]']           \n",
            "                                                                                                  \n",
            " tf.expand_dims_2 (TFOpLambda)  (None, 128, 1)       0           ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " tf.expand_dims_3 (TFOpLambda)  (None, 128, 1)       0           ['dropout_6[0][0]']              \n",
            "                                                                                                  \n",
            " transformer_block_1 (Transform  (None, 128, 128)    20611       ['tf.expand_dims_2[0][0]',       \n",
            " erBlock)                                                         'tf.expand_dims_3[0][0]']       \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 16384)        0           ['transformer_block_1[0][0]']    \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 128)          2097280     ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " tf.nn.gelu_1 (TFOpLambda)      (None, 128)          0           ['dense_11[0][0]']               \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 128)          16512       ['tf.nn.gelu_1[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 128)          0           ['dense_12[0][0]']               \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 128)          0           ['dense_11[0][0]',               \n",
            "                                                                  'dropout_9[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 256)          0           ['dropout_5[0][0]',              \n",
            "                                                                  'dropout_6[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_5 (LayerNo  (None, 128)         256         ['add_1[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " attention_1 (Attention)        (None, 128)          1           ['dropout_5[0][0]',              \n",
            "                                                                  'dropout_6[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 512)          0           ['concatenate_2[0][0]',          \n",
            "                                                                  'layer_normalization_5[0][0]',  \n",
            "                                                                  'attention_1[0][0]']            \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 1)            513         ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 24,886,468\n",
            "Trainable params: 2,364,805\n",
            "Non-trainable params: 22,521,663\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "DDAXg6GZF7tP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "-EvQQlevMUbP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multimodal_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(), loss=loss, metrics=metrics\n",
        ")\n",
        "history = multimodal_model.fit(train_ds, validation_data=test_ds, epochs=epochs, batch_size=batch_size, callbacks=[save_model1])"
      ],
      "metadata": {
        "id": "nHJ6ptX7NzOb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "881b0965-0f6c-43f6-fcaf-e049a3b6f1b0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "446/446 [==============================] - 73s 125ms/step - loss: 0.4847 - accuracy: 0.7829 - precision: 0.7333 - recall: 0.5796 - val_loss: 0.5140 - val_accuracy: 0.8622 - val_precision: 0.9337 - val_recall: 0.8401\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 52s 117ms/step - loss: 0.3617 - accuracy: 0.8446 - precision: 0.8048 - recall: 0.7238 - val_loss: 0.5299 - val_accuracy: 0.8066 - val_precision: 0.9381 - val_recall: 0.7407\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 52s 117ms/step - loss: 0.3190 - accuracy: 0.8651 - precision: 0.8306 - recall: 0.7635 - val_loss: 0.5874 - val_accuracy: 0.7103 - val_precision: 0.9357 - val_recall: 0.5783\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 52s 117ms/step - loss: 0.3047 - accuracy: 0.8712 - precision: 0.8369 - recall: 0.7772 - val_loss: 0.5438 - val_accuracy: 0.7733 - val_precision: 0.9457 - val_recall: 0.6777\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 52s 116ms/step - loss: 0.2813 - accuracy: 0.8850 - precision: 0.8551 - recall: 0.8014 - val_loss: 0.4946 - val_accuracy: 0.8435 - val_precision: 0.9521 - val_recall: 0.7904\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 53s 118ms/step - loss: 0.2663 - accuracy: 0.8905 - precision: 0.8667 - recall: 0.8057 - val_loss: 0.4253 - val_accuracy: 0.8710 - val_precision: 0.9469 - val_recall: 0.8418\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 53s 118ms/step - loss: 0.2672 - accuracy: 0.8915 - precision: 0.8620 - recall: 0.8151 - val_loss: 0.3976 - val_accuracy: 0.8768 - val_precision: 0.9466 - val_recall: 0.8517\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 53s 118ms/step - loss: 0.2536 - accuracy: 0.8951 - precision: 0.8659 - recall: 0.8226 - val_loss: 0.3788 - val_accuracy: 0.8820 - val_precision: 0.9399 - val_recall: 0.8674\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 52s 116ms/step - loss: 0.2417 - accuracy: 0.8999 - precision: 0.8743 - recall: 0.8281 - val_loss: 0.3793 - val_accuracy: 0.8773 - val_precision: 0.9378 - val_recall: 0.8616\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 52s 117ms/step - loss: 0.2316 - accuracy: 0.9062 - precision: 0.8841 - recall: 0.8369 - val_loss: 0.4197 - val_accuracy: 0.8710 - val_precision: 0.9444 - val_recall: 0.8442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score"
      ],
      "metadata": {
        "id": "JvqG8xphbPgD"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = np.concatenate([y for x, y in test_ds], axis=0)"
      ],
      "metadata": {
        "id": "YLi_UnaxKmpt"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = multimodal_model.predict(test_ds)\n",
        "y_pred = [1 if i>=0.5 else 0 for i in y_pred]\n",
        "print(f'Accuracy == {accuracy_score(y_test,y_pred)}')\n",
        "print(f'F1 == {f1_score(y_test,y_pred,average=None)}')\n",
        "print(f'Precision == {precision_score(y_test,y_pred,average=None)}')\n",
        "print(f'Recall == {recall_score(y_test,y_pred,average=None)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCZsFJUaM4ze",
        "outputId": "d028879e-5605-46ae-e538-b60d8cf9041d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "61/61 [==============================] - 9s 99ms/step\n",
            "Accuracy == 0.8772750910036401\n",
            "F1 == [0.84655397 0.89774697]\n",
            "Precision == [0.7919708  0.94096276]\n",
            "Recall == [0.90921788 0.85832643]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multimodal_model.load_weights('/content/drive/MyDrive/multimodal-news/twitter-models/model-comb3-weights.0.8820-0.38.hdf5')"
      ],
      "metadata": {
        "id": "WFnVbhMIN6O_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = multimodal_model.predict(test_ds)\n",
        "y_pred = [1 if i>=0.5 else 0 for i in y_pred]\n",
        "print(f'Accuracy == {accuracy_score(y_test,y_pred)}')\n",
        "print(f'F1 == {f1_score(y_test,y_pred,average=None)}')\n",
        "print(f'Precision == {precision_score(y_test,y_pred,average=None)}')\n",
        "print(f'Recall == {recall_score(y_test,y_pred,average=None)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvkYNwRhN9LQ",
        "outputId": "3d61c1ac-79f3-41b0-d455-32bb9cb70d32"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "61/61 [==============================] - 8s 93ms/step\n",
            "Accuracy == 0.8819552782111284\n",
            "F1 == [0.85114754 0.90219733]\n",
            "Precision == [0.80222497 0.93985637]\n",
            "Recall == [0.90642458 0.86743993]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J3iL7oSiHvtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = '/content/drive/MyDrive/multimodal-news/twitter-models/model-comb4-weights.{val_accuracy:.4f}-{val_loss:.2f}.hdf5'\n",
        "save_model2 = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)"
      ],
      "metadata": {
        "id": "y8ACY9bVDroK"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multimodal_model = create_multimodal_model(attention=True, num_projection_layers=1, projection_dims=128, dropout_rate=0.2)\n",
        "multimodal_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "905d89cb-efbe-4c94-9caf-9bf84f48e575",
        "id": "Yf9t9ktuDroT"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " image_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " input_mask (InputLayer)        [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " input_type_ids (InputLayer)    [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " input_word_ids (InputLayer)    [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " vision_encoder (Functional)    (None, 128)          13144126    ['image_1[0][0]']                \n",
            "                                                                                                  \n",
            " text_encoder (Functional)      (None, 128)          9640705     ['input_mask[0][0]',             \n",
            "                                                                  'input_type_ids[0][0]',         \n",
            "                                                                  'input_word_ids[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 128)          0           ['vision_encoder[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 128)          0           ['text_encoder[0][0]']           \n",
            "                                                                                                  \n",
            " tf.expand_dims_4 (TFOpLambda)  (None, 128, 1)       0           ['dropout_12[0][0]']             \n",
            "                                                                                                  \n",
            " tf.expand_dims_5 (TFOpLambda)  (None, 128, 1)       0           ['dropout_13[0][0]']             \n",
            "                                                                                                  \n",
            " transformer_block_2 (Transform  (None, 128, 128)    20611       ['tf.expand_dims_4[0][0]',       \n",
            " erBlock)                                                         'tf.expand_dims_5[0][0]']       \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 16384)        0           ['transformer_block_2[0][0]']    \n",
            "                                                                                                  \n",
            " dense_20 (Dense)               (None, 128)          2097280     ['flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " tf.nn.gelu_4 (TFOpLambda)      (None, 128)          0           ['dense_20[0][0]']               \n",
            "                                                                                                  \n",
            " dense_21 (Dense)               (None, 128)          16512       ['tf.nn.gelu_4[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 128)          0           ['dense_21[0][0]']               \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 128)          0           ['dense_20[0][0]',               \n",
            "                                                                  'dropout_16[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 256)          0           ['dropout_12[0][0]',             \n",
            "                                                                  'dropout_13[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_10 (LayerN  (None, 128)         256         ['add_4[0][0]']                  \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " attention_2 (Attention)        (None, 128)          1           ['dropout_12[0][0]',             \n",
            "                                                                  'dropout_13[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 512)          0           ['concatenate_4[0][0]',          \n",
            "                                                                  'layer_normalization_10[0][0]', \n",
            "                                                                  'attention_2[0][0]']            \n",
            "                                                                                                  \n",
            " dense_22 (Dense)               (None, 1)            513         ['concatenate_5[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 24,920,004\n",
            "Trainable params: 2,398,341\n",
            "Non-trainable params: 22,521,663\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "I_Rx8_g_DroT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "sTiyA8v6DroT"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multimodal_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(), loss=loss, metrics=metrics\n",
        ")\n",
        "history = multimodal_model.fit(train_ds, validation_data=test_ds, epochs=epochs, batch_size=batch_size, callbacks=[save_model2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "569b60d4-e238-4267-db8d-e15c60418d8e",
        "id": "IJEJt8WrDroT"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "446/446 [==============================] - 76s 127ms/step - loss: 0.4952 - accuracy: 0.7801 - precision: 0.7505 - recall: 0.6257 - val_loss: 0.4386 - val_accuracy: 0.8211 - val_precision: 0.8611 - val_recall: 0.8525\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 54s 120ms/step - loss: 0.3548 - accuracy: 0.8471 - precision: 0.8027 - recall: 0.7366 - val_loss: 0.4371 - val_accuracy: 0.8414 - val_precision: 0.9396 - val_recall: 0.7987\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 53s 120ms/step - loss: 0.3133 - accuracy: 0.8675 - precision: 0.8267 - recall: 0.7780 - val_loss: 0.3783 - val_accuracy: 0.8658 - val_precision: 0.8977 - val_recall: 0.8873\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 53s 119ms/step - loss: 0.2679 - accuracy: 0.8896 - precision: 0.8552 - recall: 0.8175 - val_loss: 0.3346 - val_accuracy: 0.8929 - val_precision: 0.9473 - val_recall: 0.8782\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 53s 118ms/step - loss: 0.2490 - accuracy: 0.8994 - precision: 0.8722 - recall: 0.8290 - val_loss: 0.3565 - val_accuracy: 0.8695 - val_precision: 0.8729 - val_recall: 0.9271\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 53s 118ms/step - loss: 0.2245 - accuracy: 0.9073 - precision: 0.8817 - recall: 0.8436 - val_loss: 0.3741 - val_accuracy: 0.8825 - val_precision: 0.9189 - val_recall: 0.8915\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 52s 118ms/step - loss: 0.2127 - accuracy: 0.9145 - precision: 0.8940 - recall: 0.8526 - val_loss: 0.5833 - val_accuracy: 0.7702 - val_precision: 0.9412 - val_recall: 0.6761\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 52s 118ms/step - loss: 0.1942 - accuracy: 0.9229 - precision: 0.9018 - recall: 0.8705 - val_loss: 0.5301 - val_accuracy: 0.7993 - val_precision: 0.9477 - val_recall: 0.7200\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 52s 118ms/step - loss: 0.1856 - accuracy: 0.9273 - precision: 0.9103 - recall: 0.8748 - val_loss: 0.4271 - val_accuracy: 0.8424 - val_precision: 0.9371 - val_recall: 0.8028\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 52s 118ms/step - loss: 0.1686 - accuracy: 0.9341 - precision: 0.9196 - recall: 0.8860 - val_loss: 0.3799 - val_accuracy: 0.8612 - val_precision: 0.9417 - val_recall: 0.8302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-6FK7tiMD77v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = '/content/drive/MyDrive/multimodal-news/twitter-models/model-comb5-weights.{val_accuracy:.4f}-{val_loss:.2f}.hdf5'\n",
        "save_model3 = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)"
      ],
      "metadata": {
        "id": "qzkPcIWtIucc"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multimodal_model = create_multimodal_model(attention=True, num_projection_layers=1, projection_dims=128, dropout_rate=0.1)"
      ],
      "metadata": {
        "id": "ALVLTXl-Iucc"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "kMSAHcPkIucc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "W_5jiIPdIucd"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multimodal_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(), loss=loss, metrics=metrics\n",
        ")\n",
        "history = multimodal_model.fit(train_ds, validation_data=test_ds, epochs=epochs, batch_size=batch_size, callbacks=[save_model3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c55a6d2f-9ec0-42f7-fefc-acd4baffab01",
        "id": "qbrBwVXXIucd"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "446/446 [==============================] - 75s 126ms/step - loss: 0.4538 - accuracy: 0.8040 - precision: 0.7904 - recall: 0.6548 - val_loss: 0.4645 - val_accuracy: 0.8383 - val_precision: 0.9409 - val_recall: 0.7920\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 53s 120ms/step - loss: 0.3365 - accuracy: 0.8585 - precision: 0.8161 - recall: 0.7600 - val_loss: 0.3814 - val_accuracy: 0.8539 - val_precision: 0.8640 - val_recall: 0.9105\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 53s 118ms/step - loss: 0.2988 - accuracy: 0.8758 - precision: 0.8386 - recall: 0.7912 - val_loss: 0.4704 - val_accuracy: 0.8346 - val_precision: 0.9559 - val_recall: 0.7722\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 53s 119ms/step - loss: 0.2634 - accuracy: 0.8895 - precision: 0.8590 - recall: 0.8122 - val_loss: 0.3762 - val_accuracy: 0.8809 - val_precision: 0.9445 - val_recall: 0.8608\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 53s 119ms/step - loss: 0.2320 - accuracy: 0.9041 - precision: 0.8794 - recall: 0.8357 - val_loss: 0.3453 - val_accuracy: 0.8825 - val_precision: 0.9504 - val_recall: 0.8575\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 53s 120ms/step - loss: 0.2175 - accuracy: 0.9105 - precision: 0.8898 - recall: 0.8444 - val_loss: 0.3475 - val_accuracy: 0.8877 - val_precision: 0.8911 - val_recall: 0.9354\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 53s 118ms/step - loss: 0.2031 - accuracy: 0.9198 - precision: 0.9024 - recall: 0.8597 - val_loss: 0.4379 - val_accuracy: 0.8523 - val_precision: 0.9341 - val_recall: 0.8227\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 52s 118ms/step - loss: 0.1876 - accuracy: 0.9283 - precision: 0.9118 - recall: 0.8762 - val_loss: 0.3431 - val_accuracy: 0.8679 - val_precision: 0.9360 - val_recall: 0.8476\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 54s 120ms/step - loss: 0.1733 - accuracy: 0.9351 - precision: 0.9209 - recall: 0.8875 - val_loss: 0.3269 - val_accuracy: 0.8892 - val_precision: 0.9360 - val_recall: 0.8840\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 53s 118ms/step - loss: 0.1680 - accuracy: 0.9353 - precision: 0.9186 - recall: 0.8909 - val_loss: 0.3307 - val_accuracy: 0.8820 - val_precision: 0.9321 - val_recall: 0.8757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Enq9vXOkI-Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = multimodal_model.predict(test_ds)\n",
        "y_pred = [1 if i>=0.5 else 0 for i in y_pred]\n",
        "print(f'Accuracy == {accuracy_score(y_test,y_pred)}')\n",
        "print(f'F1 == {f1_score(y_test,y_pred,average=None)}')\n",
        "print(f'Precision == {precision_score(y_test,y_pred,average=None)}')\n",
        "print(f'Recall == {recall_score(y_test,y_pred,average=None)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d833db-2747-40a1-cc1a-8ca4dfee8575",
        "id": "95MmNUoZLuaY"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "61/61 [==============================] - 8s 92ms/step\n",
            "Accuracy == 0.8819552782111284\n",
            "F1 == [0.84916944 0.90303289]\n",
            "Precision == [0.80988593 0.93209877]\n",
            "Recall == [0.8924581  0.87572494]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multimodal_model.load_weights('/content/drive/MyDrive/multimodal-news/twitter-models/model-comb5-weights.0.8877-0.35.hdf5')"
      ],
      "metadata": {
        "id": "o93tBYInLuaY"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = multimodal_model.predict(test_ds)\n",
        "y_pred = [1 if i>=0.5 else 0 for i in y_pred]\n",
        "print(f'Accuracy == {accuracy_score(y_test,y_pred)}')\n",
        "print(f'F1 == {f1_score(y_test,y_pred,average=None)}')\n",
        "print(f'Precision == {precision_score(y_test,y_pred,average=None)}')\n",
        "print(f'Recall == {recall_score(y_test,y_pred,average=None)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba0bfc0-185e-4b69-a7d2-e29b4b2e9913",
        "id": "tUx8CrZdLuaY"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "61/61 [==============================] - 6s 94ms/step\n",
            "Accuracy == 0.8876755070202809\n",
            "F1 == [0.8425656 0.912692 ]\n",
            "Precision == [0.88109756 0.89108129]\n",
            "Recall == [0.80726257 0.93537697]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This is the best model we have got on Twitter dataset**"
      ],
      "metadata": {
        "id": "Y77AUn8GNRGy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O96YP8KHNO6R"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}