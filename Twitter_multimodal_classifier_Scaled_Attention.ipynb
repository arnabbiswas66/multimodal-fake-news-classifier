{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnabbiswas66/multimodal-fake-news-classifier/blob/main/Twitter_multimodal_classifier_Scaled_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AECm9YZxm322",
        "outputId": "4b7209c6-7813-4173-852d-88a07dfee0fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for chances (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "#!pip install -q -U \"tensorflow-text==2.11.*\"\n",
        "!pip install -q tensorflow_text\n",
        "!pip install -q talos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S0_Dwt-ondpi"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.platform.tf_logging import warn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import re\n",
        "from os import listdir\n",
        "import shutil\n",
        "import glob\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import talos as ta\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x20211H_nnLR",
        "outputId": "382bf0c0-fe8e-4953-95e0-6a1b4bc423ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BbHqdujoUn87"
      },
      "outputs": [],
      "source": [
        "BASE_SAVE_LOCATION = \"/content/drive/MyDrive/multimodal-news\"\n",
        "CHECKPOINT_FILEPATH = '/content/drive/MyDrive/multimodal-news/twitter-models/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKusTHhYWBlz"
      },
      "source": [
        "## Load the dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JftdaRkIWAAo",
        "outputId": "934fc161-850c-4751-936b-b8df60b55ead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14258, 3) (1923, 3)\n"
          ]
        }
      ],
      "source": [
        "#Load training df\n",
        "train_df_model = pd.read_pickle(BASE_SAVE_LOCATION+\"/twitter-train.pickle\")\n",
        "test_df_model = pd.read_pickle(BASE_SAVE_LOCATION+\"/twitter-test.pickle\")\n",
        "print(train_df_model.shape, test_df_model.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "g3iaPhB7gqdq"
      },
      "outputs": [],
      "source": [
        "# This is for Talos as it doesn't take TF DataSet as input\n",
        "X_train = train_df_model.copy()\n",
        "y_train = X_train.pop('label')\n",
        "\n",
        "X_test = test_df_model.copy()\n",
        "y_test = X_test.pop('label')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCuEsRz4CIdR"
      },
      "source": [
        "# Data input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sjc2FLKhBfbj"
      },
      "outputs": [],
      "source": [
        "# Define TF Hub paths to the BERT encoder and its preprocessor\n",
        "bert_model_path = (\n",
        "    \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1\"\n",
        ")\n",
        "bert_preprocess_path = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uZzOdgKYInBJ"
      },
      "outputs": [],
      "source": [
        "def make_bert_preprocessing_model(sentence_features, seq_length=128):\n",
        "    \"\"\"Returns Model mapping string features to BERT inputs.\n",
        "\n",
        "  Args:\n",
        "    sentence_features: A list with the names of string-valued features.\n",
        "    seq_length: An integer that defines the sequence length of BERT inputs.\n",
        "\n",
        "  Returns:\n",
        "    A Keras Model that can be called on a list or dict of string Tensors\n",
        "    (with the order or names, resp., given by sentence_features) and\n",
        "    returns a dict of tensors for input to BERT.\n",
        "  \"\"\"\n",
        "\n",
        "    input_segments = [\n",
        "        tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
        "        for ft in sentence_features\n",
        "    ]\n",
        "\n",
        "    # Tokenize the text to word pieces.\n",
        "    bert_preprocess = hub.load(bert_preprocess_path)\n",
        "    tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name=\"tokenizer\")\n",
        "    segments = [tokenizer(s) for s in input_segments]\n",
        "\n",
        "    # Optional: Trim segments in a smart way to fit seq_length.\n",
        "    # Simple cases (like this example) can skip this step and let\n",
        "    # the next step apply a default truncation to approximately equal lengths.\n",
        "    truncated_segments = segments\n",
        "\n",
        "    # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
        "    # are model-dependent, so this gets loaded from the SavedModel.\n",
        "    packer = hub.KerasLayer(\n",
        "        bert_preprocess.bert_pack_inputs,\n",
        "        arguments=dict(seq_length=seq_length),\n",
        "        name=\"packer\",\n",
        "    )\n",
        "    model_inputs = packer(truncated_segments)\n",
        "    return keras.Model(input_segments, model_inputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqW4X6onJQKQ",
        "outputId": "6adb5c5c-2b6a-44c1-b055-f074405951af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_1 (InputLayer)         [(None,)]                 0         \n",
            "                                                                 \n",
            " tokenizer (KerasLayer)      (None, None, None)        0         \n",
            "                                                                 \n",
            " packer (KerasLayer)         {'input_word_ids': (None  0         \n",
            "                             , 128),                             \n",
            "                              'input_mask': (None, 12            \n",
            "                             8),                                 \n",
            "                              'input_type_ids': (None            \n",
            "                             , 128)}                             \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 0\n",
            "Trainable params: 0\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "bert_preprocess_model = make_bert_preprocessing_model([\"text_1\"])\n",
        "bert_preprocess_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCOmGQXtJn9a",
        "outputId": "5c863775-1f6d-4342-9172-fc99a282fa0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1: This is Breezy Point, Queens as seen by @TomKaminskiWCBS in Chopper 880 this morning. http://t.co/kt8o8NwS #Sandy http://t.co/NRnW8O0s\n",
            "Text 2: This is Breezy Point, Queens as seen by @TomKaminskiWCBS in Chopper 880 this morning. http://t.co/kt8o8NwS #Sandy http://t.co/NRnW8O0s\n",
            "Keys           :  ['input_word_ids', 'input_mask', 'input_type_ids']\n",
            "Shape Word Ids :  (1, 128)\n",
            "Word Ids       :  tf.Tensor(\n",
            "[  101  2023  2003 21986  9096  2391  1010  8603  2004  2464  2011  1030\n",
            "  3419 27052 19880 16526], shape=(16,), dtype=int32)\n",
            "Shape Mask     :  (1, 128)\n",
            "Input Mask     :  tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(16,), dtype=int32)\n",
            "Shape Type Ids :  (1, 128)\n",
            "Type Ids       :  tf.Tensor([0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(16,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "idx = np.random.choice(len(train_df_model))\n",
        "row = train_df_model.iloc[idx]\n",
        "sample_text_1, sample_text_2 = row[\"tweetText\"], row[\"tweetText\"]\n",
        "print(f\"Text 1: {sample_text_1}\")\n",
        "print(f\"Text 2: {sample_text_2}\")\n",
        "\n",
        "test_text = [np.array([sample_text_1])]\n",
        "text_preprocessed = bert_preprocess_model(test_text)\n",
        "\n",
        "print(\"Keys           : \", list(text_preprocessed.keys()))\n",
        "print(\"Shape Word Ids : \", text_preprocessed[\"input_word_ids\"].shape)\n",
        "print(\"Word Ids       : \", text_preprocessed[\"input_word_ids\"][0, :16])\n",
        "print(\"Shape Mask     : \", text_preprocessed[\"input_mask\"].shape)\n",
        "print(\"Input Mask     : \", text_preprocessed[\"input_mask\"][0, :16])\n",
        "print(\"Shape Type Ids : \", text_preprocessed[\"input_type_ids\"].shape)\n",
        "print(\"Type Ids       : \", text_preprocessed[\"input_type_ids\"][0, :16])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dhn-DPb7Lsb1"
      },
      "outputs": [],
      "source": [
        "def dataframe_to_dataset(dataframe):\n",
        "    columns = ['tweetText', 'image_1', 'label']\n",
        "    dataframe = dataframe[columns].copy()\n",
        "    labels = dataframe.pop(\"label\")\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO6IRl9rkfT4"
      },
      "source": [
        "## Preprocessing utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "slef7sY9ke1l"
      },
      "outputs": [],
      "source": [
        "resize = (224, 224)\n",
        "bert_input_features = [\"input_word_ids\", \"input_type_ids\", \"input_mask\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FrTCJPUzkP3Q"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path):\n",
        "  extension = tf.strings.split(image_path,'.')[-1]\n",
        "  image = tf.io.read_file(image_path)\n",
        "  if extension == b\"gif\":\n",
        "    image = tf.io.decode_image(image, 3, expand_animations=False)\n",
        "  elif extension == b\"png\":\n",
        "    image = tf.image.decode_png(image, 3)\n",
        "  else:\n",
        "    image = tf.image.decode_jpeg(image, 3)\n",
        "  image = tf.image.resize(image, resize)\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IvgkLXK5k5ZX"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text_1):\n",
        "  text_1 = tf.convert_to_tensor([text_1])\n",
        "  output = bert_preprocess_model([text_1])\n",
        "  output = {feature: tf.squeeze(output[feature]) for feature in bert_input_features}\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uXQHweTHpCQM"
      },
      "outputs": [],
      "source": [
        "def preprocess_text_and_image(sample):\n",
        "  image_1 = preprocess_image(sample[\"image_1\"])\n",
        "  text = preprocess_text(sample[\"tweetText\"])\n",
        "  return {\"image_1\": image_1, \"text\": text}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "C-0aLqtupoL5"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "auto = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DwL2IjrIp-Tn"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(dataframe, training=True):\n",
        "  ds = dataframe_to_dataset(dataframe)\n",
        "  if training:\n",
        "      ds = ds.shuffle(len(train_df_model))\n",
        "  ds = ds.map(lambda x, y: (preprocess_text_and_image(x), y)).cache()\n",
        "  ds = ds.batch(batch_size).prefetch(auto)\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5rSseL-fqSCE"
      },
      "outputs": [],
      "source": [
        "train_ds = prepare_dataset(train_df_model)\n",
        "test_ds = prepare_dataset(test_df_model, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUA1fjql-NCs"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsnHD-at_70f"
      },
      "source": [
        "### Projection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JUmnESU36Q-N"
      },
      "outputs": [],
      "source": [
        "def project_embeddings(\n",
        "    embeddings, num_projection_layers, projection_dims, dropout_rate\n",
        "):\n",
        "    projected_embeddings = keras.layers.Dense(units=projection_dims)(embeddings)\n",
        "    for _ in range(num_projection_layers):\n",
        "        x = tf.nn.gelu(projected_embeddings)\n",
        "        x = keras.layers.Dense(projection_dims)(x)\n",
        "        x = keras.layers.Dropout(dropout_rate)(x)\n",
        "        x = keras.layers.Add()([projected_embeddings, x])\n",
        "        projected_embeddings = keras.layers.LayerNormalization()(x)\n",
        "    return projected_embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEelNTYZAB0h"
      },
      "source": [
        "### Vision encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "y2ypqRWE_Oyq"
      },
      "outputs": [],
      "source": [
        "def create_vision_encoder(\n",
        "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
        "):\n",
        "    # Load the pre-trained ResNet50V2 model to be used as the base encoder.\n",
        "    resnet_v2 = keras.applications.EfficientNetV2B3(\n",
        "        include_top=False, weights=\"imagenet\", pooling=\"avg\"\n",
        "    )\n",
        "    # Set the trainability of the base encoder.\n",
        "    for layer in resnet_v2.layers:\n",
        "        layer.trainable = trainable\n",
        "\n",
        "    # Receive the images as inputs.\n",
        "    image_1 = keras.Input(shape=(224, 224, 3), name=\"image_1\")\n",
        "    \n",
        "    # Preprocess the input image.\n",
        "    preprocessed_1 = keras.applications.resnet_v2.preprocess_input(image_1)\n",
        "    \n",
        "    # Generate the embeddings for the images using the resnet_v2 model\n",
        "    # concatenate them.\n",
        "    embeddings = resnet_v2(preprocessed_1)\n",
        "    #embeddings = keras.layers.Concatenate()([embeddings_1, embeddings_2])\n",
        "\n",
        "    # Project the embeddings produced by the model.\n",
        "    outputs = project_embeddings(\n",
        "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
        "    )\n",
        "    # Create the vision encoder model.\n",
        "    return keras.Model([image_1], outputs, name=\"vision_encoder\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtChuDndAGLm"
      },
      "source": [
        "### Text Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "UoVTKoTt_1Pv"
      },
      "outputs": [],
      "source": [
        "def create_text_encoder(\n",
        "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
        "):\n",
        "    # Load the pre-trained BERT model to be used as the base encoder.\n",
        "    bert = hub.KerasLayer(bert_model_path, name=\"bert\",)\n",
        "    # Set the trainability of the base encoder.\n",
        "    bert.trainable = trainable\n",
        "\n",
        "    # Receive the text as inputs.\n",
        "    bert_input_features = [\"input_type_ids\", \"input_mask\", \"input_word_ids\"]\n",
        "    inputs = {\n",
        "        feature: keras.Input(shape=(128,), dtype=tf.int32, name=feature)\n",
        "        for feature in bert_input_features\n",
        "    }\n",
        "\n",
        "    # Generate embeddings for the preprocessed text using the BERT model.\n",
        "    embeddings = bert(inputs)[\"pooled_output\"]\n",
        "\n",
        "    # Project the embeddings produced by the model.\n",
        "    outputs = project_embeddings(\n",
        "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
        "    )\n",
        "    # Create the text encoder model.\n",
        "    return keras.Model(inputs, outputs, name=\"text_encoder\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF0jLUzqjwFv"
      },
      "source": [
        "### Multi Head Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "UX52i1Pj4piV"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = keras.layers.Dropout(rate)\n",
        "        self.dropout2 = keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, y, training):\n",
        "        attn_output = self.att(x, y)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugj-e4g8AWOp"
      },
      "source": [
        "## MultiModal model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1RjaNwDnANoc"
      },
      "outputs": [],
      "source": [
        "def create_multimodal_model(\n",
        "    num_projection_layers=0,\n",
        "    projection_dims=224,\n",
        "    dropout_rate=0.1,\n",
        "    vision_trainable=False,\n",
        "    text_trainable=False,\n",
        "    attention=False\n",
        "):\n",
        "    # Receive the images as inputs.\n",
        "    image_1 = keras.Input(shape=(224, 224, 3), name=\"image_1\")\n",
        "    \n",
        "    # Receive the text as inputs.\n",
        "    bert_input_features = [\"input_type_ids\", \"input_mask\", \"input_word_ids\"]\n",
        "    text_inputs = {\n",
        "        feature: keras.Input(shape=(128,), dtype=tf.int32, name=feature)\n",
        "        for feature in bert_input_features\n",
        "    }\n",
        "\n",
        "    # Create the encoders.\n",
        "    vision_encoder = create_vision_encoder(\n",
        "        num_projection_layers, projection_dims, dropout_rate, vision_trainable\n",
        "    )\n",
        "    text_encoder = create_text_encoder(\n",
        "        num_projection_layers, projection_dims, dropout_rate, text_trainable\n",
        "    )\n",
        "\n",
        "    # Fetch the embedding projections.\n",
        "    vision_projections = vision_encoder([image_1])\n",
        "    vision_projections = keras.layers.Dropout(dropout_rate)(vision_projections)\n",
        "    text_projections = text_encoder(text_inputs)\n",
        "    text_projections = keras.layers.Dropout(dropout_rate)(text_projections)\n",
        "    \n",
        "    # Cross-attention.\n",
        "    if attention:\n",
        "      transformer_block = TransformerBlock(projection_dims, 4, projection_dims)\n",
        "      x = transformer_block(tf.expand_dims(vision_projections, -1), tf.expand_dims(text_projections, -1))\n",
        "      x = tf.keras.layers.Flatten()(x)\n",
        "      \n",
        "    # Concatenate the projections and pass through the classification layer.\n",
        "    concatenated = keras.layers.Concatenate()([vision_projections, text_projections])\n",
        "    if attention:\n",
        "        concatenated = keras.layers.Concatenate()([concatenated, x])\n",
        "        #x = tf.keras.layers.Flatten(x)\n",
        "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(concatenated)\n",
        "    return keras.Model([image_1, text_inputs], outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "HYAqM03bYqpj"
      },
      "outputs": [],
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics= [\n",
        "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      tf.keras.metrics.Precision(name='precision'),\n",
        "      tf.keras.metrics.Recall(name='recall')\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lzh_oeuuZVrA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvh6vbUTXRRs"
      },
      "source": [
        "## Final Multimodal model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "iMyfwuhtXQRJ"
      },
      "outputs": [],
      "source": [
        "def multimodal_model(X_train, y_train, X_test, y_test, params):\n",
        "  train_ds = prepare_dataset(train_df_model)\n",
        "  test_ds = prepare_dataset(test_df_model, False)\n",
        "  model = create_multimodal_model(params['num_projection_layers'],\n",
        "    params['projection_dims'],\n",
        "    params['dropout_rate'],\n",
        "    params['vision_trainable'],\n",
        "    params['text_trainable'],\n",
        "    params['attention'])\n",
        "  model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(params['lr']), loss=loss, metrics=metrics\n",
        "  )\n",
        "  history = model.fit(\n",
        "      train_ds, validation_data=test_ds, \n",
        "      epochs=params['epochs'], batch_size=params['batch_size'])\n",
        "  return history, model\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_hLDJku6bY9O"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'num_projection_layers' : [0],\n",
        "    'projection_dims' : [128, 224],\n",
        "    'dropout_rate' : [0.1, 0.2],\n",
        "    'vision_trainable' : [False],\n",
        "    'text_trainable' : [False],\n",
        "    'attention' : [True],\n",
        "    'lr' : [0.001, 0.0005],\n",
        "    'epochs' : [10],\n",
        "    'batch_size' : [32, 64]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhrpIETIevhH",
        "outputId": "79224f02-f43c-4d89-c097-fc20bc27ea89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/16 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_projection_layers': 0, 'projection_dims': 128, 'dropout_rate': 0.1, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.001, 'epochs': 10, 'batch_size': 32}\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b3_notop.h5\n",
            "52606240/52606240 [==============================] - 3s 0us/step\n",
            "Epoch 1/10\n",
            "446/446 [==============================] - 383s 763ms/step - loss: 0.5123 - accuracy: 0.7680 - precision: 0.7019 - recall: 0.5660 - val_loss: 0.4329 - val_accuracy: 0.8294 - val_precision: 0.8347 - val_recall: 0.9080\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 52s 117ms/step - loss: 0.3573 - accuracy: 0.8514 - precision: 0.8119 - recall: 0.7392 - val_loss: 0.7081 - val_accuracy: 0.5642 - val_precision: 0.9165 - val_recall: 0.3364\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 53s 118ms/step - loss: 0.3124 - accuracy: 0.8681 - precision: 0.8357 - recall: 0.7674 - val_loss: 0.5549 - val_accuracy: 0.7639 - val_precision: 0.9414 - val_recall: 0.6653\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 53s 120ms/step - loss: 0.2864 - accuracy: 0.8832 - precision: 0.8549 - recall: 0.7953 - val_loss: 0.6544 - val_accuracy: 0.6401 - val_precision: 0.9357 - val_recall: 0.4582\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 54s 120ms/step - loss: 0.2749 - accuracy: 0.8887 - precision: 0.8605 - recall: 0.8073 - val_loss: 0.4763 - val_accuracy: 0.8534 - val_precision: 0.9263 - val_recall: 0.8326\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2628 - accuracy: 0.8930 - precision: 0.8671 - recall: 0.8139 - val_loss: 0.6412 - val_accuracy: 0.7051 - val_precision: 0.9444 - val_recall: 0.5634\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2441 - accuracy: 0.8986 - precision: 0.8715 - recall: 0.8271 - val_loss: 0.6816 - val_accuracy: 0.6563 - val_precision: 0.9446 - val_recall: 0.4805\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 54s 120ms/step - loss: 0.2384 - accuracy: 0.9050 - precision: 0.8812 - recall: 0.8365 - val_loss: 0.4509 - val_accuracy: 0.8710 - val_precision: 0.9562 - val_recall: 0.8326\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2383 - accuracy: 0.9023 - precision: 0.8781 - recall: 0.8314 - val_loss: 0.5822 - val_accuracy: 0.7852 - val_precision: 0.9563 - val_recall: 0.6893\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2298 - accuracy: 0.9069 - precision: 0.8854 - recall: 0.8379 - val_loss: 0.5708 - val_accuracy: 0.7956 - val_precision: 0.9583 - val_recall: 0.7051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▋         | 1/16 [15:08<3:47:14, 908.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_projection_layers': 0, 'projection_dims': 128, 'dropout_rate': 0.1, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.001, 'epochs': 10, 'batch_size': 64}\n",
            "Epoch 1/10\n",
            "446/446 [==============================] - 105s 200ms/step - loss: 0.4749 - accuracy: 0.7901 - precision: 0.7823 - recall: 0.6157 - val_loss: 0.5123 - val_accuracy: 0.8248 - val_precision: 0.9207 - val_recall: 0.7887\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 53s 120ms/step - loss: 0.3414 - accuracy: 0.8578 - precision: 0.8203 - recall: 0.7511 - val_loss: 0.5712 - val_accuracy: 0.7239 - val_precision: 0.9322 - val_recall: 0.6040\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 54s 120ms/step - loss: 0.2996 - accuracy: 0.8792 - precision: 0.8494 - recall: 0.7886 - val_loss: 0.5979 - val_accuracy: 0.7077 - val_precision: 0.9436 - val_recall: 0.5684\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2750 - accuracy: 0.8870 - precision: 0.8565 - recall: 0.8067 - val_loss: 0.4712 - val_accuracy: 0.8440 - val_precision: 0.9512 - val_recall: 0.7920\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2723 - accuracy: 0.8886 - precision: 0.8601 - recall: 0.8073 - val_loss: 0.7867 - val_accuracy: 0.5959 - val_precision: 0.9300 - val_recall: 0.3853\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2556 - accuracy: 0.8973 - precision: 0.8702 - recall: 0.8245 - val_loss: 0.8608 - val_accuracy: 0.5689 - val_precision: 0.9315 - val_recall: 0.3380\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2385 - accuracy: 0.9055 - precision: 0.8835 - recall: 0.8353 - val_loss: 0.5521 - val_accuracy: 0.8144 - val_precision: 0.9550 - val_recall: 0.7390\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2353 - accuracy: 0.9066 - precision: 0.8840 - recall: 0.8387 - val_loss: 0.7180 - val_accuracy: 0.6615 - val_precision: 0.9455 - val_recall: 0.4888\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2306 - accuracy: 0.9087 - precision: 0.8900 - recall: 0.8381 - val_loss: 0.4863 - val_accuracy: 0.8388 - val_precision: 0.9526 - val_recall: 0.7821\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2244 - accuracy: 0.9101 - precision: 0.8887 - recall: 0.8444 - val_loss: 0.4893 - val_accuracy: 0.8378 - val_precision: 0.9562 - val_recall: 0.7771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▎        | 2/16 [25:08<2:49:36, 726.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_projection_layers': 0, 'projection_dims': 128, 'dropout_rate': 0.1, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.0005, 'epochs': 10, 'batch_size': 32}\n",
            "Epoch 1/10\n",
            "446/446 [==============================] - 106s 201ms/step - loss: 0.5123 - accuracy: 0.7665 - precision: 0.7533 - recall: 0.5679 - val_loss: 0.4781 - val_accuracy: 0.7447 - val_precision: 0.7146 - val_recall: 0.9876\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 53s 120ms/step - loss: 0.3733 - accuracy: 0.8419 - precision: 0.8039 - recall: 0.7148 - val_loss: 0.4305 - val_accuracy: 0.7988 - val_precision: 0.7812 - val_recall: 0.9437\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.3277 - accuracy: 0.8633 - precision: 0.8282 - recall: 0.7604 - val_loss: 0.4304 - val_accuracy: 0.8445 - val_precision: 0.8498 - val_recall: 0.9138\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.3020 - accuracy: 0.8761 - precision: 0.8454 - recall: 0.7829 - val_loss: 0.4065 - val_accuracy: 0.8565 - val_precision: 0.8745 - val_recall: 0.9006\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2892 - accuracy: 0.8785 - precision: 0.8477 - recall: 0.7886 - val_loss: 0.4338 - val_accuracy: 0.8658 - val_precision: 0.9025 - val_recall: 0.8815\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 54s 120ms/step - loss: 0.2733 - accuracy: 0.8894 - precision: 0.8625 - recall: 0.8071 - val_loss: 0.3833 - val_accuracy: 0.8851 - val_precision: 0.9228 - val_recall: 0.8915\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 54s 120ms/step - loss: 0.2591 - accuracy: 0.8944 - precision: 0.8697 - recall: 0.8151 - val_loss: 0.4233 - val_accuracy: 0.8762 - val_precision: 0.9457 - val_recall: 0.8517\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2498 - accuracy: 0.8998 - precision: 0.8768 - recall: 0.8245 - val_loss: 0.3608 - val_accuracy: 0.8851 - val_precision: 0.9280 - val_recall: 0.8857\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 53s 120ms/step - loss: 0.2363 - accuracy: 0.9067 - precision: 0.8877 - recall: 0.8345 - val_loss: 0.3664 - val_accuracy: 0.8679 - val_precision: 0.8928 - val_recall: 0.8973\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 53s 120ms/step - loss: 0.2369 - accuracy: 0.9076 - precision: 0.8868 - recall: 0.8385 - val_loss: 0.3599 - val_accuracy: 0.8653 - val_precision: 0.8854 - val_recall: 0.9022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▉        | 3/16 [35:36<2:27:40, 681.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_projection_layers': 0, 'projection_dims': 128, 'dropout_rate': 0.1, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.0005, 'epochs': 10, 'batch_size': 64}\n",
            "Epoch 1/10\n",
            "446/446 [==============================] - 106s 202ms/step - loss: 0.5207 - accuracy: 0.7637 - precision: 0.7369 - recall: 0.5825 - val_loss: 0.5045 - val_accuracy: 0.8180 - val_precision: 0.9506 - val_recall: 0.7490\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 53s 120ms/step - loss: 0.3725 - accuracy: 0.8435 - precision: 0.8095 - recall: 0.7129 - val_loss: 0.7011 - val_accuracy: 0.4867 - val_precision: 0.8767 - val_recall: 0.2121\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.3323 - accuracy: 0.8611 - precision: 0.8259 - recall: 0.7554 - val_loss: 0.6024 - val_accuracy: 0.6828 - val_precision: 0.9332 - val_recall: 0.5327\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.3042 - accuracy: 0.8735 - precision: 0.8421 - recall: 0.7784 - val_loss: 0.6808 - val_accuracy: 0.6115 - val_precision: 0.9291 - val_recall: 0.4126\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 54s 120ms/step - loss: 0.2842 - accuracy: 0.8829 - precision: 0.8536 - recall: 0.7963 - val_loss: 0.4626 - val_accuracy: 0.8742 - val_precision: 0.9423 - val_recall: 0.8517\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2652 - accuracy: 0.8947 - precision: 0.8726 - recall: 0.8126 - val_loss: 0.5552 - val_accuracy: 0.7863 - val_precision: 0.9533 - val_recall: 0.6935\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2577 - accuracy: 0.8968 - precision: 0.8720 - recall: 0.8206 - val_loss: 0.4472 - val_accuracy: 0.8716 - val_precision: 0.9436 - val_recall: 0.8459\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2495 - accuracy: 0.8991 - precision: 0.8741 - recall: 0.8255 - val_loss: 0.5308 - val_accuracy: 0.8274 - val_precision: 0.9572 - val_recall: 0.7589\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2393 - accuracy: 0.9029 - precision: 0.8780 - recall: 0.8336 - val_loss: 0.4714 - val_accuracy: 0.8653 - val_precision: 0.9566 - val_recall: 0.8227\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2386 - accuracy: 0.9024 - precision: 0.8781 - recall: 0.8316 - val_loss: 0.3997 - val_accuracy: 0.8768 - val_precision: 0.9338 - val_recall: 0.8650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 4/16 [46:04<2:12:09, 660.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_projection_layers': 0, 'projection_dims': 128, 'dropout_rate': 0.2, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.001, 'epochs': 10, 'batch_size': 32}\n",
            "Epoch 1/10\n",
            "446/446 [==============================] - 107s 203ms/step - loss: 0.4982 - accuracy: 0.7837 - precision: 0.7533 - recall: 0.6355 - val_loss: 0.5641 - val_accuracy: 0.7634 - val_precision: 0.9372 - val_recall: 0.6678\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.3534 - accuracy: 0.8506 - precision: 0.8109 - recall: 0.7378 - val_loss: 0.5330 - val_accuracy: 0.7925 - val_precision: 0.9307 - val_recall: 0.7233\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.3085 - accuracy: 0.8724 - precision: 0.8391 - recall: 0.7782 - val_loss: 0.7931 - val_accuracy: 0.4805 - val_precision: 0.8881 - val_recall: 0.1972\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2937 - accuracy: 0.8770 - precision: 0.8435 - recall: 0.7888 - val_loss: 0.5022 - val_accuracy: 0.8430 - val_precision: 0.9511 - val_recall: 0.7904\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2819 - accuracy: 0.8861 - precision: 0.8569 - recall: 0.8031 - val_loss: 0.5713 - val_accuracy: 0.7161 - val_precision: 0.9436 - val_recall: 0.5824\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2636 - accuracy: 0.8926 - precision: 0.8688 - recall: 0.8102 - val_loss: 0.5278 - val_accuracy: 0.8144 - val_precision: 0.9541 - val_recall: 0.7399\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2522 - accuracy: 0.8968 - precision: 0.8711 - recall: 0.8214 - val_loss: 0.6802 - val_accuracy: 0.6448 - val_precision: 0.9411 - val_recall: 0.4631\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2429 - accuracy: 0.9029 - precision: 0.8818 - recall: 0.8287 - val_loss: 0.5251 - val_accuracy: 0.8320 - val_precision: 0.9595 - val_recall: 0.7647\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2334 - accuracy: 0.9072 - precision: 0.8872 - recall: 0.8367 - val_loss: 0.5924 - val_accuracy: 0.7540 - val_precision: 0.9531 - val_recall: 0.6396\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2388 - accuracy: 0.9034 - precision: 0.8764 - recall: 0.8371 - val_loss: 0.4820 - val_accuracy: 0.8502 - val_precision: 0.9590 - val_recall: 0.7954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███▏      | 5/16 [56:34<1:59:06, 649.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_projection_layers': 0, 'projection_dims': 128, 'dropout_rate': 0.2, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.001, 'epochs': 10, 'batch_size': 64}\n",
            "Epoch 1/10\n",
            "446/446 [==============================] - 108s 205ms/step - loss: 0.4855 - accuracy: 0.7841 - precision: 0.7619 - recall: 0.6230 - val_loss: 0.7174 - val_accuracy: 0.4815 - val_precision: 0.8777 - val_recall: 0.2022\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 53s 120ms/step - loss: 0.3522 - accuracy: 0.8506 - precision: 0.8087 - recall: 0.7411 - val_loss: 0.5419 - val_accuracy: 0.7832 - val_precision: 0.9399 - val_recall: 0.6993\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.3105 - accuracy: 0.8740 - precision: 0.8402 - recall: 0.7825 - val_loss: 0.6314 - val_accuracy: 0.6599 - val_precision: 0.9396 - val_recall: 0.4896\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2902 - accuracy: 0.8796 - precision: 0.8467 - recall: 0.7939 - val_loss: 0.5272 - val_accuracy: 0.8196 - val_precision: 0.9604 - val_recall: 0.7432\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 54s 122ms/step - loss: 0.2807 - accuracy: 0.8843 - precision: 0.8538 - recall: 0.8010 - val_loss: 0.5028 - val_accuracy: 0.8388 - val_precision: 0.9535 - val_recall: 0.7813\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 54s 122ms/step - loss: 0.2611 - accuracy: 0.8949 - precision: 0.8678 - recall: 0.8194 - val_loss: 0.4603 - val_accuracy: 0.8648 - val_precision: 0.9531 - val_recall: 0.8252\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 54s 122ms/step - loss: 0.2572 - accuracy: 0.8937 - precision: 0.8691 - recall: 0.8135 - val_loss: 0.4104 - val_accuracy: 0.8788 - val_precision: 0.9501 - val_recall: 0.8517\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 54s 122ms/step - loss: 0.2439 - accuracy: 0.9019 - precision: 0.8785 - recall: 0.8298 - val_loss: 0.6779 - val_accuracy: 0.6609 - val_precision: 0.9498 - val_recall: 0.4855\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 54s 122ms/step - loss: 0.2424 - accuracy: 0.9013 - precision: 0.8750 - recall: 0.8320 - val_loss: 0.5013 - val_accuracy: 0.8450 - val_precision: 0.9605 - val_recall: 0.7854\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 54s 122ms/step - loss: 0.2312 - accuracy: 0.9073 - precision: 0.8840 - recall: 0.8408 - val_loss: 0.4066 - val_accuracy: 0.8669 - val_precision: 0.9391 - val_recall: 0.8426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 6/16 [1:07:07<1:47:19, 643.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_projection_layers': 0, 'projection_dims': 128, 'dropout_rate': 0.2, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.0005, 'epochs': 10, 'batch_size': 32}\n",
            "Epoch 1/10\n",
            "446/446 [==============================] - 107s 204ms/step - loss: 0.5021 - accuracy: 0.7674 - precision: 0.7512 - recall: 0.5746 - val_loss: 0.5294 - val_accuracy: 0.8196 - val_precision: 0.9526 - val_recall: 0.7498\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 53s 119ms/step - loss: 0.3658 - accuracy: 0.8468 - precision: 0.8144 - recall: 0.7182 - val_loss: 0.6870 - val_accuracy: 0.5751 - val_precision: 0.9167 - val_recall: 0.3554\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.3340 - accuracy: 0.8585 - precision: 0.8221 - recall: 0.7511 - val_loss: 0.5951 - val_accuracy: 0.6698 - val_precision: 0.9320 - val_recall: 0.5112\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.3087 - accuracy: 0.8701 - precision: 0.8371 - recall: 0.7729 - val_loss: 0.6169 - val_accuracy: 0.6589 - val_precision: 0.9325 - val_recall: 0.4921\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2882 - accuracy: 0.8821 - precision: 0.8527 - recall: 0.7945 - val_loss: 0.6229 - val_accuracy: 0.6646 - val_precision: 0.9336 - val_recall: 0.5012\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 54s 120ms/step - loss: 0.2723 - accuracy: 0.8874 - precision: 0.8590 - recall: 0.8049 - val_loss: 0.6289 - val_accuracy: 0.6630 - val_precision: 0.9374 - val_recall: 0.4963\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2619 - accuracy: 0.8901 - precision: 0.8592 - recall: 0.8139 - val_loss: 0.4141 - val_accuracy: 0.8742 - val_precision: 0.9251 - val_recall: 0.8699\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2515 - accuracy: 0.9008 - precision: 0.8802 - recall: 0.8239 - val_loss: 0.4810 - val_accuracy: 0.8450 - val_precision: 0.9559 - val_recall: 0.7896\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2441 - accuracy: 0.9022 - precision: 0.8777 - recall: 0.8314 - val_loss: 0.4782 - val_accuracy: 0.8430 - val_precision: 0.9557 - val_recall: 0.7862\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.2358 - accuracy: 0.9050 - precision: 0.8796 - recall: 0.8387 - val_loss: 0.4004 - val_accuracy: 0.8799 - val_precision: 0.9444 - val_recall: 0.8592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 7/16 [1:17:09<1:34:31, 630.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_projection_layers': 0, 'projection_dims': 128, 'dropout_rate': 0.2, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.0005, 'epochs': 10, 'batch_size': 64}\n",
            "Epoch 1/10\n",
            "446/446 [==============================] - 111s 215ms/step - loss: 0.5366 - accuracy: 0.7535 - precision: 0.7246 - recall: 0.5605 - val_loss: 0.4396 - val_accuracy: 0.8326 - val_precision: 0.9403 - val_recall: 0.7829\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 54s 120ms/step - loss: 0.3821 - accuracy: 0.8332 - precision: 0.7943 - recall: 0.6952 - val_loss: 0.4335 - val_accuracy: 0.8315 - val_precision: 0.8409 - val_recall: 0.9022\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 54s 121ms/step - loss: 0.3442 - accuracy: 0.8565 - precision: 0.8190 - recall: 0.7482 - val_loss: 0.4249 - val_accuracy: 0.8310 - val_precision: 0.8382 - val_recall: 0.9056\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 55s 122ms/step - loss: 0.3145 - accuracy: 0.8664 - precision: 0.8295 - recall: 0.7698 - val_loss: 0.4240 - val_accuracy: 0.8809 - val_precision: 0.9187 - val_recall: 0.8890\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 55s 123ms/step - loss: 0.2992 - accuracy: 0.8760 - precision: 0.8466 - recall: 0.7810 - val_loss: 0.4123 - val_accuracy: 0.8814 - val_precision: 0.9216 - val_recall: 0.8865\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 55s 122ms/step - loss: 0.2757 - accuracy: 0.8875 - precision: 0.8611 - recall: 0.8024 - val_loss: 0.4159 - val_accuracy: 0.8528 - val_precision: 0.8696 - val_recall: 0.9006\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 55s 122ms/step - loss: 0.2651 - accuracy: 0.8904 - precision: 0.8614 - recall: 0.8120 - val_loss: 0.3939 - val_accuracy: 0.8606 - val_precision: 0.8808 - val_recall: 0.8998\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - ETA: 0s - loss: 0.2589 - accuracy: 0.8928 - precision: 0.8662 - recall: 0.8143"
          ]
        }
      ],
      "source": [
        "h = ta.Scan(x = X_train, y= y_train, params = params, model = multimodal_model, x_val = X_test, y_val = y_test, experiment_name = 'multi1', save_weights=False, print_params=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'num_projection_layers' : [1],\n",
        "    'projection_dims' : [128, 224],\n",
        "    'dropout_rate' : [0.1, 0.2],\n",
        "    'vision_trainable' : [False],\n",
        "    'text_trainable' : [False],\n",
        "    'attention' : [True],\n",
        "    'lr' : [0.001, 0.0005],\n",
        "    'epochs' : [10],\n",
        "    'batch_size' : [32]\n",
        "}"
      ],
      "metadata": {
        "id": "wC_IOmzcm2BE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = ta.Scan(x = X_train, y= y_train, params = params, model = multimodal_model, x_val = X_test, y_val = y_test, experiment_name = 'multi1', save_weights=False, print_params=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNyODIYMm-mo",
        "outputId": "495afa3f-c151-4574-b752-01a4e2e04726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/8 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_projection_layers': 1, 'projection_dims': 128, 'dropout_rate': 0.1, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.001, 'epochs': 10, 'batch_size': 32}\n",
            "Epoch 1/10\n",
            "446/446 [==============================] - 120s 208ms/step - loss: 0.4790 - accuracy: 0.7883 - precision: 0.7164 - recall: 0.6365 - val_loss: 0.4602 - val_accuracy: 0.8346 - val_precision: 0.9397 - val_recall: 0.7871\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 57s 127ms/step - loss: 0.3078 - accuracy: 0.8746 - precision: 0.8324 - recall: 0.7957 - val_loss: 0.5246 - val_accuracy: 0.7400 - val_precision: 0.9164 - val_recall: 0.6446\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 57s 129ms/step - loss: 0.2658 - accuracy: 0.8926 - precision: 0.8580 - recall: 0.8241 - val_loss: 0.4445 - val_accuracy: 0.8383 - val_precision: 0.8972 - val_recall: 0.8384\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 58s 130ms/step - loss: 0.2417 - accuracy: 0.9045 - precision: 0.8772 - recall: 0.8400 - val_loss: 0.4927 - val_accuracy: 0.8076 - val_precision: 0.8893 - val_recall: 0.7920\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 58s 130ms/step - loss: 0.2144 - accuracy: 0.9150 - precision: 0.8903 - recall: 0.8587 - val_loss: 0.8454 - val_accuracy: 0.5377 - val_precision: 0.8750 - val_recall: 0.3074\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 58s 130ms/step - loss: 0.1980 - accuracy: 0.9228 - precision: 0.9030 - recall: 0.8689 - val_loss: 0.5192 - val_accuracy: 0.7592 - val_precision: 0.7735 - val_recall: 0.8716\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 58s 130ms/step - loss: 0.1970 - accuracy: 0.9226 - precision: 0.9039 - recall: 0.8671 - val_loss: 0.6551 - val_accuracy: 0.6745 - val_precision: 0.8797 - val_recall: 0.5576\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 58s 130ms/step - loss: 0.1761 - accuracy: 0.9318 - precision: 0.9153 - recall: 0.8834 - val_loss: 1.0252 - val_accuracy: 0.4899 - val_precision: 0.7989 - val_recall: 0.2502\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 58s 130ms/step - loss: 0.1642 - accuracy: 0.9374 - precision: 0.9248 - recall: 0.8903 - val_loss: 1.5750 - val_accuracy: 0.4685 - val_precision: 0.7577 - val_recall: 0.2254\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 58s 131ms/step - loss: 0.1586 - accuracy: 0.9393 - precision: 0.9258 - recall: 0.8954 - val_loss: 2.5752 - val_accuracy: 0.4394 - val_precision: 0.7295 - val_recall: 0.1698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▎        | 1/8 [10:52<1:16:05, 652.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_projection_layers': 1, 'projection_dims': 128, 'dropout_rate': 0.1, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.0005, 'epochs': 10, 'batch_size': 32}\n",
            "Epoch 1/10\n",
            "446/446 [==============================] - 107s 200ms/step - loss: 0.4708 - accuracy: 0.7456 - precision: 0.7220 - recall: 0.5311 - val_loss: 0.5192 - val_accuracy: 0.8242 - val_precision: 0.9550 - val_recall: 0.7556\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 58s 129ms/step - loss: 0.3308 - accuracy: 0.8614 - precision: 0.8151 - recall: 0.7723 - val_loss: 0.5360 - val_accuracy: 0.8232 - val_precision: 0.9549 - val_recall: 0.7539\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 58s 130ms/step - loss: 0.2863 - accuracy: 0.8812 - precision: 0.8415 - recall: 0.8065 - val_loss: 0.7739 - val_accuracy: 0.5299 - val_precision: 0.8895 - val_recall: 0.2867\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 58s 131ms/step - loss: 0.2506 - accuracy: 0.9015 - precision: 0.8731 - recall: 0.8349 - val_loss: 0.8268 - val_accuracy: 0.5757 - val_precision: 0.9186 - val_recall: 0.3554\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 58s 131ms/step - loss: 0.2449 - accuracy: 0.9030 - precision: 0.8750 - recall: 0.8377 - val_loss: 0.9972 - val_accuracy: 0.4457 - val_precision: 0.8373 - val_recall: 0.1450\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 58s 130ms/step - loss: 0.2265 - accuracy: 0.9113 - precision: 0.8848 - recall: 0.8532 - val_loss: 1.2990 - val_accuracy: 0.4155 - val_precision: 0.7943 - val_recall: 0.0928\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 58s 129ms/step - loss: 0.2144 - accuracy: 0.9172 - precision: 0.8945 - recall: 0.8610 - val_loss: 1.0238 - val_accuracy: 0.4561 - val_precision: 0.8286 - val_recall: 0.1682\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 58s 130ms/step - loss: 0.2020 - accuracy: 0.9189 - precision: 0.8970 - recall: 0.8632 - val_loss: 1.2393 - val_accuracy: 0.4373 - val_precision: 0.7990 - val_recall: 0.1384\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 58s 129ms/step - loss: 0.2004 - accuracy: 0.9186 - precision: 0.8966 - recall: 0.8628 - val_loss: 0.5422 - val_accuracy: 0.7769 - val_precision: 0.8220 - val_recall: 0.8227\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 58s 130ms/step - loss: 0.1821 - accuracy: 0.9280 - precision: 0.9100 - recall: 0.8777 - val_loss: 1.7901 - val_accuracy: 0.4124 - val_precision: 0.7484 - val_recall: 0.0961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 2/8 [22:19<1:07:15, 672.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_projection_layers': 1, 'projection_dims': 128, 'dropout_rate': 0.2, 'vision_trainable': False, 'text_trainable': False, 'attention': True, 'lr': 0.001, 'epochs': 10, 'batch_size': 32}\n",
            "Epoch 1/10\n",
            "125/446 [=======>......................] - ETA: 58s - loss: 0.6397 - accuracy: 0.5994 - precision: 0.5882 - recall: 0.2947"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}